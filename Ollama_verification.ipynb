{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec0e387",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f6f639",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dbda3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm_response(prompt):\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    \n",
    "    data = {\n",
    "        \"model\": \"llama3.3:70b-instruct-q8_0\",\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    \n",
    "    response = requests.post(url, json=data)\n",
    "    return response.json()['response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dde0451",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests\n",
    "!pip install typing-extensions==4.7.1\n",
    "!pip install pydantic==2.4.2\n",
    "!pip install ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8620844e",
   "metadata": {},
   "source": [
    "KSR Vol 1 - Chunk 2000, Overlap 300"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317e2473",
   "metadata": {},
   "source": [
    "KSR Vol 1 - Chunk 2000, Overlap 300"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4c5b12",
   "metadata": {},
   "source": [
    "KSR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ca809d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing text chunks:   0%|‚ñè                                                                                                                 | 1/659 [00:44<8:05:20, 44.26s/it]"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_section_info(text):\n",
    "    patterns = {\n",
    "        'Document': r'Document:\"([^\"]+)\"',\n",
    "        'Part': r'Part:\"([^\"]+)\"',\n",
    "        'Chapter': r'Chapter:\"([^\"]+)\"',\n",
    "        'Appendix': r'Appendix:\"([^\"]+)\"',\n",
    "        'Annexure': r'Annexure:\"([^\"]+)\"',\n",
    "        'Section': r'Section:\"([^\"]+)\"',\n",
    "        'Sub Section': r'Sub Section:\"([^\"]+)\"',\n",
    "        'Sub division': r'Sub division:\"([^\"]+)\"',\n",
    "        'Rule no.': r'Rule no.:\"([^\"]+)\"'\n",
    "    }\n",
    "    \n",
    "    result = {k: None for k in patterns.keys()}\n",
    "    \n",
    "    for key, pattern in patterns.items():\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            result[key] = match.group(1)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def process_text_chunk(text):\n",
    "    # Split into sections based on \"Rule no.:\" pattern\n",
    "    sections = re.split(r'(?=Rule no.:\")', text)\n",
    "    sections = [s for s in sections if s.strip()]\n",
    "\n",
    "    results = []\n",
    "    for section in sections:\n",
    "        # Extract section info and description\n",
    "        section_info = extract_section_info(section)\n",
    "        \n",
    "        # Extract description - everything after the last known field\n",
    "        description_pattern = r'Description:\"([^\"]+)\"'\n",
    "        desc_match = re.search(description_pattern, section)\n",
    "        if desc_match:\n",
    "            section_info['Description'] = desc_match.group(1)\n",
    "        \n",
    "        if any(section_info.values()):\n",
    "            results.append(section_info)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def chunk_text(text, max_length=2000, overlap=300):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    \n",
    "    while start < len(text):\n",
    "        end = start + max_length\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        start = end - overlap\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def process_with_llama(text):\n",
    "    prompt = \"\"\"You are tasked with extracting structured data from a text block that contains information about rules or sections from a document, likely the Kerala Service Rules (KSR). Your goal is to parse this information and format it as a JSON list.\n",
    "\n",
    "The JSON format for each entry should be as follows:\n",
    "{\n",
    "  \"Document\": \"KSR\",\n",
    "  \"Part\": \"\",\n",
    "  \"Chapter\": \"\",\n",
    "  \"Appendix\": \"\",\n",
    "  \"Annexure\": \"\",\n",
    "  \"Section\": \"\",\n",
    "  \"Sub Section\": \"\",\n",
    "  \"Sub division\": \"\",\n",
    "  \"Rule no.\": \"\",\n",
    "  \"Description\": \"\"\n",
    "}\n",
    "\n",
    "Here is the text block you need to parse:\n",
    "\n",
    "<text_block>\n",
    "{?text}\n",
    "</text_block>\n",
    "\n",
    "To extract the required information:\n",
    "\n",
    "1. Identify the document name (usually \"KSR\" unless specified otherwise).\n",
    "2. Look for indicators of Part, Chapter, Appendix, Annexure, Section, Sub Section, Sub division, and Rule number. These may be explicitly stated or implied by the structure of the text.\n",
    "3. The remaining text should be considered as the Description.\n",
    "4. If any field is not present in the text, leave it as an empty string in the JSON.\n",
    "5. If multiple rules or sections are present in the text block, create separate JSON objects for each.\n",
    "6. Ensure that the Rule number is a string, even if it's a number.\n",
    "7. For the Description, include all relevant text, including notes, government decisions, and examples if present.\n",
    "8. Do not discard any part of the text block. If you cannot classify any portion of the text according to the specified fields (Part, Chapter, Appendix, Annexure, Section, Sub Section, Sub division, Rule number), include it in the Description field and populate its remaining attributes (Part, Chapter, Appendix, Annexure, Section, Sub Section, Sub division, Rule number) with the corresponding attributes from the previous JSON object, because these fields are mandatory. Do not put random stuff in these field, always put the correct ones. \n",
    "9. Format the Rule no. field exactly as shown in the template above.\n",
    "\n",
    "Your output should be a valid JSON list containing one or more objects in the specified format. Do not include any additional text or explanations outside of the JSON structure.\n",
    "\n",
    "<output>\n",
    "</output>\n",
    "\"\"\".replace(\"{?text}\", text)\n",
    "\n",
    "    response = requests.post('http://localhost:11434/api/generate',\n",
    "                           json={\n",
    "                               'model': 'llama3.3:70b-instruct-fp16',\n",
    "                               'prompt': prompt,\n",
    "                               'stream': False\n",
    "                           })\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        try:\n",
    "            response_json = response.json()['response']\n",
    "            # Extract JSON content from markdown code block if present\n",
    "            if '```json' in response_json:\n",
    "                json_str = response_json.split('```json')[1].split('```')[0].strip()\n",
    "            else:\n",
    "                json_str = response_json.strip()\n",
    "            \n",
    "            # Parse the JSON string\n",
    "            return json.loads(json_str)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing LLM response: {e}\")\n",
    "            print(f\"Response was: {response.json()['response']}\")\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def main():\n",
    "    # Read the input file\n",
    "#     with open('Extracted/KSR_Vol_2_pdfminer_extracted.txt', 'r', encoding='utf-8') as file:\n",
    "#         text = file.read()\n",
    "        \n",
    "    # Open and read both files\n",
    "    with open('Extracted/KSR_Vol_1_pdfminer_extracted.txt', 'r', encoding='utf-8') as file1, \\\n",
    "         open('Extracted/KSR_Vol_2_pdfminer_extracted.txt', 'r', encoding='utf-8') as file2:\n",
    "        text = file1.read() + file2.read()\n",
    "        \n",
    "    \n",
    "    # Process text in chunks\n",
    "    chunks = chunk_text(text)  # Removed the [:3] slice to process all chunks\n",
    "    all_results = []\n",
    "    \n",
    "    for chunk in tqdm(chunks, desc=\"Processing text chunks\"):\n",
    "        # Try direct pattern matching first\n",
    "        results = process_text_chunk(chunk)\n",
    "        \n",
    "        # If pattern matching fails, use LLM\n",
    "        if not results:\n",
    "            llm_result = process_with_llama(chunk)\n",
    "            if llm_result:\n",
    "                all_results.extend(llm_result if isinstance(llm_result, list) else [llm_result])\n",
    "        else:\n",
    "            all_results.extend(results)\n",
    "    \n",
    "    # Add debug output\n",
    "    print(f\"Total results found: {len(all_results)}\")\n",
    "    \n",
    "    # Write results to JSON file\n",
    "    if all_results:\n",
    "        with open('FP_Full_KSR_extracted_rules2.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(all_results, f, indent=4, ensure_ascii=False)\n",
    "        print(\"Results successfully written to Full_KSR_extracted_rules.json\")\n",
    "    else:\n",
    "        print(\"No results were found to write to file\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    result = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fe3727",
   "metadata": {},
   "source": [
    "KFC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb93286",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_section_info(text):\n",
    "    patterns = {\n",
    "        'Document': r'Document:\"([^\"]+)\"',\n",
    "        'Chapter': r'Chapter:\"([^\"]+)\"',\n",
    "        'Appendix': r'Appendix:\"([^\"]+)\"',\n",
    "        'Annexure': r'Annexure:\"([^\"]+)\"',\n",
    "        'Rule no.': r'Rule no.:\"([^\"]+)\"'\n",
    "    }\n",
    "    \n",
    "    result = {k: None for k in patterns.keys()}\n",
    "    \n",
    "    for key, pattern in patterns.items():\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            result[key] = match.group(1)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def process_text_chunk(text):\n",
    "    # Split into sections based on \"Rule no.:\" pattern\n",
    "    sections = re.split(r'(?=Rule no.:\")', text)\n",
    "    sections = [s for s in sections if s.strip()]\n",
    "\n",
    "    results = []\n",
    "    for section in sections:\n",
    "        # Extract section info and description\n",
    "        section_info = extract_section_info(section)\n",
    "        \n",
    "        # Extract description - everything after the last known field\n",
    "        description_pattern = r'Description:\"([^\"]+)\"'\n",
    "        desc_match = re.search(description_pattern, section)\n",
    "        if desc_match:\n",
    "            section_info['Description'] = desc_match.group(1)\n",
    "        \n",
    "        if any(section_info.values()):\n",
    "            results.append(section_info)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def chunk_text(text, max_length=2000, overlap=300):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    \n",
    "    while start < len(text):\n",
    "        end = start + max_length\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        start = end - overlap\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def process_with_llama(text):\n",
    "    prompt = \"\"\"You are tasked with extracting structured data from a text block that contains information about rules from a document, which is Kerala Financial Code Vol I. Your goal is to parse this information and format it as a JSON list.\n",
    "\n",
    "The JSON format for each entry should be as follows:\n",
    "{\n",
    "  \"Document\": \"KFC\",\n",
    "  \"Chapter\": \"\",\n",
    "  \"Appendix\": \"\",\n",
    "  \"Annexure\": \"\",\n",
    "  \"Rule no.\": \"\",\n",
    "  \"Description\": \"\"\n",
    "}\n",
    "\n",
    "Here is the text block you need to parse:\n",
    "\n",
    "<text_block>\n",
    "{?text}\n",
    "</text_block>\n",
    "\n",
    "To extract the required information:\n",
    "\n",
    "1. Identify the document name (usually \"KFC\" unless specified otherwise).\n",
    "2. Look for indicators of Chapter, Appendix, Annexure, and Rule number. These may be explicitly stated or implied by the structure of the text.\n",
    "3. The remaining text should be considered as the Description.\n",
    "4. If any field is not present in the text, leave it as an empty string in the JSON.\n",
    "5. If multiple rules or sections are present in the text block, create separate JSON objects for each.\n",
    "6. Ensure that the Rule number is a string, even if it's a number.\n",
    "7. For the Description, include all relevant text, including notes, government decisions, and examples if present.\n",
    "8. Do not discard any part of the text block. If you cannot classify any portion of the text according to the specified fields (Chapter, Appendix, Annexure, Rule no.), include it in the Description field.\n",
    "9. Format the Rule no. field exactly as shown in the template above.\n",
    "\n",
    "Your output should be a valid JSON list containing one or more objects in the specified format. Do not include any additional text or explanations outside of the JSON structure.\n",
    "\n",
    "<output>\n",
    "</output>\n",
    "\"\"\".replace(\"{?text}\", text)\n",
    "\n",
    "    response = requests.post('http://localhost:11434/api/generate',\n",
    "                           json={\n",
    "                               'model': 'llama3.3:70b-instruct-fp16',\n",
    "                               'prompt': prompt,\n",
    "                               'stream': False\n",
    "                           })\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        try:\n",
    "            response_json = response.json()['response']\n",
    "            # Extract JSON content from markdown code block if present\n",
    "            # Clean up the response\n",
    "            if '```json' in response_json:\n",
    "                json_str = response_json.split('```json')[1].split('```')[0].strip()\n",
    "            else:\n",
    "                json_str = response_json.strip()\n",
    "            \n",
    "            # Remove any leading/trailing whitespace or newlines\n",
    "            json_str = json_str.strip()\n",
    "            \n",
    "            # Parse the JSON string\n",
    "            return json.loads(json_str)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing LLM response: {e}\")\n",
    "            print(f\"Response was: {response.json()['response']}\")\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def main():\n",
    "    # Read the input file\n",
    "    with open('/workspace/rohith_llm/Extracted/KFC1_pdfminer_extracted.txt', 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "        \n",
    "            \n",
    "    \n",
    "    # Process text in chunks\n",
    "    chunks = chunk_text(text)  # Removed the [:3] slice to process all chunks\n",
    "    all_results = []\n",
    "    \n",
    "    for chunk in tqdm(chunks, desc=\"Processing text chunks\"):\n",
    "        # Try direct pattern matching first\n",
    "        results = process_text_chunk(chunk)\n",
    "        \n",
    "        # If pattern matching fails, use LLM\n",
    "        if not results:\n",
    "            llm_result = process_with_llama(chunk)\n",
    "            if llm_result:\n",
    "                all_results.extend(llm_result if isinstance(llm_result, list) else [llm_result])\n",
    "        else:\n",
    "            all_results.extend(results)\n",
    "    \n",
    "    # Add debug output\n",
    "    print(f\"Total results found: {len(all_results)}\")\n",
    "    \n",
    "    # Write results to JSON file\n",
    "    if all_results:\n",
    "        with open('FP_Full_KFC_extracted_rules.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(all_results, f, indent=4, ensure_ascii=False)\n",
    "        print(\"Results successfully written to Full_KFC_extracted_rules.json\")\n",
    "    else:\n",
    "        print(\"No results were found to write to file\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    result = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24558eba",
   "metadata": {},
   "source": [
    "KTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f01f82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_section_info(text):\n",
    "    patterns = {\n",
    "        'Document': r'Document:\"([^\"]+)\"',\n",
    "        'Part': r'Part:\"([^\"]+)\"',\n",
    "        'Section': r'Section:\"([^\"]+)\"',\n",
    "        'Rule no.': r'Rule no.:\"([^\"]+)\"'\n",
    "    }\n",
    "    \n",
    "    result = {k: None for k in patterns.keys()}\n",
    "    \n",
    "    for key, pattern in patterns.items():\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            result[key] = match.group(1)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def process_text_chunk(text):\n",
    "    # Split into sections based on \"Rule no.:\" pattern\n",
    "    sections = re.split(r'(?=Rule no.:\")', text)\n",
    "    sections = [s for s in sections if s.strip()]\n",
    "\n",
    "    results = []\n",
    "    for section in sections:\n",
    "        # Extract section info and description\n",
    "        section_info = extract_section_info(section)\n",
    "        \n",
    "        # Extract description - everything after the last known field\n",
    "        description_pattern = r'Description:\"([^\"]+)\"'\n",
    "        desc_match = re.search(description_pattern, section)\n",
    "        if desc_match:\n",
    "            section_info['Description'] = desc_match.group(1)\n",
    "        \n",
    "        if any(section_info.values()):\n",
    "            results.append(section_info)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def chunk_text(text, max_length=2000, overlap=300):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    \n",
    "    while start < len(text):\n",
    "        end = start + max_length\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        start = end - overlap\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def process_with_llama(text):\n",
    "    prompt = \"\"\"You are tasked with extracting structured data from a text block that contains information about rules from a document, which is Kerala Financial Code Vol I. Your goal is to parse this information and format it as a JSON list.\n",
    "\n",
    "The JSON format for each entry should be as follows:\n",
    "{\n",
    "  \"Document\": \"KTC\",\n",
    "  \"Part\": \"\", #always present\n",
    "  \"Section\": \"\", #present most of the time\n",
    "  \"Rule no.\": \"\", #always present\n",
    "  \"Description\": \"\"\n",
    "}\n",
    "\n",
    "Here is the text block you need to parse:\n",
    "\n",
    "<text_block>\n",
    "{?text}\n",
    "</text_block>\n",
    "\n",
    "To extract the required information:\n",
    "\n",
    "1. Identify the document name (usually \"KTC\" unless specified otherwise).\n",
    "2. Look for indicators of Part, Section, and Rule number. These may be explicitly stated or implied by the structure of the text.\n",
    "3. The remaining text should be considered as the Description.\n",
    "4. If any field is not present in the text, leave it as an empty string in the JSON.\n",
    "5. If multiple rules or sections are present in the text block, create separate JSON objects for each.\n",
    "6. Ensure that the Rule number is a string, even if it's a number.\n",
    "7. For the Description, include all relevant text, including notes, government decisions, and examples if present.\n",
    "8. Do not discard any part of the text block. If you cannot classify any portion of the text according to the specified fields (Part, Section, Rule number), include it in the Description field and populate its remaining attributes (Part, Section, and Rule number) with the corresponding attributes from the previous JSON object, because these fields are mandatory. Do not put random stuff in these field, always put the correct ones. \n",
    "9. Format the Rule no. field exactly as shown in the template above.\n",
    "\n",
    "Your output should be a valid JSON list containing one or more objects in the specified format. Do not include any additional text or explanations outside of the JSON structure.\n",
    "\n",
    "<output>\n",
    "</output>\n",
    "\"\"\".replace(\"{?text}\", text)\n",
    "\n",
    "    response = requests.post('http://localhost:11434/api/generate',\n",
    "                           json={\n",
    "                               'model': 'llama3.3:70b-instruct-fp16',\n",
    "                               'prompt': prompt,\n",
    "                               'stream': False\n",
    "                           })\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        try:\n",
    "            response_json = response.json()['response']\n",
    "            # Extract JSON content from markdown code block if present\n",
    "            # Clean up the response\n",
    "            if '```json' in response_json:\n",
    "                json_str = response_json.split('```json')[1].split('```')[0].strip()\n",
    "            else:\n",
    "                json_str = response_json.strip()\n",
    "            \n",
    "            # Remove any leading/trailing whitespace or newlines\n",
    "            json_str = json_str.strip()\n",
    "            \n",
    "            # Parse the JSON string\n",
    "            return json.loads(json_str)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing LLM response: {e}\")\n",
    "            print(f\"Response was: {response.json()['response']}\")\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def main():\n",
    "    # Read the input file\n",
    "    with open('/workspace/rohith_llm/Extracted/KTC_pdfminer_extracted.txt', 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "        \n",
    "            \n",
    "    \n",
    "    # Process text in chunks\n",
    "    chunks = chunk_text(text)  # Removed the [:3] slice to process all chunks\n",
    "    all_results = []\n",
    "    \n",
    "    for chunk in tqdm(chunks, desc=\"Processing text chunks\"):\n",
    "        # Try direct pattern matching first\n",
    "        results = process_text_chunk(chunk)\n",
    "        \n",
    "        # If pattern matching fails, use LLM\n",
    "        if not results:\n",
    "            llm_result = process_with_llama(chunk)\n",
    "            if llm_result:\n",
    "                all_results.extend(llm_result if isinstance(llm_result, list) else [llm_result])\n",
    "        else:\n",
    "            all_results.extend(results)\n",
    "    \n",
    "    # Add debug output\n",
    "    print(f\"Total results found: {len(all_results)}\")\n",
    "    \n",
    "    # Write results to JSON file\n",
    "    if all_results:\n",
    "        with open('FP_Full_KTC_extracted_rules.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(all_results, f, indent=4, ensure_ascii=False)\n",
    "        print(\"Results successfully written to Full_KTC_extracted_rules.json\")\n",
    "    else:\n",
    "        print(\"No results were found to write to file\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    result = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb0599b",
   "metadata": {},
   "source": [
    "KSSR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f1db63b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing text chunks:   0%|                                                                                                                             | 0/36 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using LLM for chunk 6 of 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing text chunks:  17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                                                                 | 6/36 [03:06<15:33, 31.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using LLM for chunk 9 of 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing text chunks:  25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                                                       | 9/36 [04:13<12:19, 27.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using LLM for chunk 10 of 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing text chunks:  28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                                                   | 10/36 [05:28<15:18, 35.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing LLM response: Expecting value: line 1 column 1 (char 0)\n",
      "Response was: ### Rules Extraction from KSSR Text Block\n",
      "\n",
      "Given the complexity and variability of the text block provided, the following Python script is designed to parse through the document and extract relevant information based on predefined rules. The script assumes that each new rule starts with a specific marker (like \"(1)\", \"(2)\", etc., or \"c\", \"d\", etc.) and attempts to identify parts, sections, annexures, and rule numbers within the text.\n",
      "\n",
      "```python\n",
      "import re\n",
      "import json\n",
      "\n",
      "# Sample text block for pars...\n",
      "Using LLM for chunk 12 of 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing text chunks:  33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                                             | 12/36 [08:03<19:32, 48.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using LLM for chunk 21 of 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing text chunks:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                | 21/36 [09:45<05:53, 23.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using LLM for chunk 22 of 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing text chunks:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                             | 22/36 [10:23<05:49, 24.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using LLM for chunk 23 of 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing text chunks:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                          | 23/36 [11:10<05:59, 27.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using LLM for chunk 24 of 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing text chunks:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                      | 24/36 [12:06<06:22, 31.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using LLM for chunk 25 of 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing text chunks:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                   | 25/36 [12:54<06:20, 34.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using LLM for chunk 26 of 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing text chunks:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                | 26/36 [13:37<06:03, 36.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using LLM for chunk 27 of 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing text chunks:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                             | 27/36 [14:29<05:58, 39.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using LLM for chunk 28 of 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing text chunks:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                         | 28/36 [15:02<05:05, 38.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using LLM for chunk 29 of 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing text chunks: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [15:53<00:00, 26.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rules found: 489\n",
      "Results successfully written to Full_KSSR_extracted_rules.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_section_info(text):\n",
    "    # This approach won't work well with the KSSR format - it's looking for exact pattern matches\n",
    "    # that don't exist in the actual document structure\n",
    "    patterns = {\n",
    "        'Document': 'KSSR',  # Default document name\n",
    "        'Part': None, \n",
    "        'Section': None,\n",
    "        'Annexure': None,\n",
    "        'Rule no.': None,\n",
    "        'Description': None\n",
    "    }\n",
    "    \n",
    "    # Look for Part information\n",
    "    part_match = re.search(r'PART\\s+([I|V|X]+)', text, re.IGNORECASE)\n",
    "    if part_match:\n",
    "        patterns['Part'] = part_match.group(1)\n",
    "    \n",
    "    # Look for Rule number\n",
    "    rule_match = re.search(r'(\\d+[A-Z]*)\\.[\\s_]*([^\\.]+)\\.', text)\n",
    "    if rule_match:\n",
    "        patterns['Rule no.'] = rule_match.group(1)\n",
    "        patterns['Description'] = rule_match.group(2).strip()\n",
    "    \n",
    "    # Look for Section information\n",
    "    section_match = re.search(r'((?:SECTION|SCHEDULE)\\s+[^\\.]+)', text, re.IGNORECASE)\n",
    "    if section_match:\n",
    "        patterns['Section'] = section_match.group(1).strip()\n",
    "    \n",
    "    # Look for Annexure\n",
    "    annexure_match = re.search(r'ANNEXURE[^\\n]*', text, re.IGNORECASE)\n",
    "    if annexure_match:\n",
    "        patterns['Annexure'] = annexure_match.group(0).strip()\n",
    "    \n",
    "    return patterns\n",
    "\n",
    "def process_rule_based(text):\n",
    "    # Split by rule patterns - looking for pattern like \"1. Title\" or \"10. Qualifications\"\n",
    "    rule_pattern = r'(\\d+[A-Z]*)\\.[\\s_]*(.*?)(?=\\n\\s*\\d+[A-Z]*\\.[\\s_]|$)'\n",
    "    rules = re.findall(rule_pattern, text, re.DOTALL)\n",
    "    \n",
    "    results = []\n",
    "    current_part = \"I\"  # Default to Part I if not found\n",
    "    current_section = \"\"\n",
    "    current_annexure = \"\"\n",
    "    \n",
    "    # First look for part information\n",
    "    part_match = re.search(r'PART\\s+([I|V|X]+)', text, re.IGNORECASE)\n",
    "    if part_match:\n",
    "        current_part = part_match.group(1)\n",
    "    \n",
    "    # Look for section or schedule\n",
    "    section_match = re.search(r'((?:SECTION|SCHEDULE)\\s+[^\\.]+)', text, re.IGNORECASE)\n",
    "    if section_match:\n",
    "        current_section = section_match.group(1).strip()\n",
    "    \n",
    "    # Look for Annexure\n",
    "    annexure_match = re.search(r'ANNEXURE[^\\n]*', text, re.IGNORECASE)\n",
    "    if annexure_match:\n",
    "        current_annexure = annexure_match.group(0).strip()\n",
    "    \n",
    "    for rule_no, description in rules:\n",
    "        # Cleanup the description\n",
    "        description = description.strip()\n",
    "        \n",
    "        # Skip empty descriptions\n",
    "        if not description:\n",
    "            continue\n",
    "        \n",
    "        result = {\n",
    "            \"Document\": \"KSSR\",\n",
    "            \"Part\": current_part,\n",
    "            \"Section\": current_section,\n",
    "            \"Annexure\": current_annexure,\n",
    "            \"Rule no.\": rule_no.strip(),\n",
    "            \"Description\": description\n",
    "        }\n",
    "        results.append(result)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def chunk_text(text, max_length=5000, overlap=500):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    \n",
    "    while start < len(text):\n",
    "        end = min(start + max_length, len(text))\n",
    "        \n",
    "        # Try to find a clean break at a rule boundary\n",
    "        if end < len(text):\n",
    "            # Look for the next rule start after the intended end point\n",
    "            next_rule = re.search(r'\\n\\s*\\d+[A-Z]*\\.[\\s_]', text[end:end+overlap])\n",
    "            if next_rule:\n",
    "                # Move end to the beginning of the next rule\n",
    "                end += next_rule.start()\n",
    "            else:\n",
    "                # Look for a paragraph break if no rule boundary\n",
    "                next_para = text[end:end+overlap].find('\\n\\n')\n",
    "                if next_para != -1:\n",
    "                    end += next_para\n",
    "        \n",
    "        chunks.append(text[start:end])\n",
    "        start = end\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def process_with_llama(text):\n",
    "    prompt = \"\"\"You are tasked with extracting structured data from a text block that contains information about rules from the Kerala State and Subordinate Services Rules (KSSR). Your goal is to parse this information and format it as a JSON list.\n",
    "\n",
    "The JSON format for each entry should be as follows:\n",
    "{\n",
    "  \"Document\": \"KSSR\",\n",
    "  \"Part\": \"\", \n",
    "  \"Section\": \"\",\n",
    "  \"Annexure\": \"\",\n",
    "  \"Rule no.\": \"\",\n",
    "  \"Description\": \"\"\n",
    "}\n",
    "\n",
    "Here is the text block you need to parse:\n",
    "\n",
    "<text_block>\n",
    "{?text}\n",
    "</text_block>\n",
    "\n",
    "Rules for extraction:\n",
    "1. Document should always be \"KSSR\".\n",
    "2. Part should be identified as \"I\", \"II\", etc. (look for \"PART I\", \"PART II\", etc.)\n",
    "3. Section should be identified if present (not all rules have sections).\n",
    "4. Annexure should be identified if present (mostly in supplementary information).\n",
    "5. Rule no. should be the rule number, like \"1\", \"2\", \"2A\", \"10\" etc.\n",
    "6. Description should be the title and content of the rule.\n",
    "7. Create a separate JSON object for each rule.\n",
    "8. Ensure all fields are present in each JSON object, even if empty.\n",
    "\n",
    "Your output should be a valid JSON array containing one object per rule identified.\n",
    "\"\"\".replace(\"{?text}\", text)\n",
    "\n",
    "    response = requests.post('http://localhost:11434/api/generate',\n",
    "                           json={\n",
    "                               'model': 'llama3.3:70b-instruct-q8_0',\n",
    "                               'prompt': prompt,\n",
    "                               'stream': False\n",
    "                           })\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        try:\n",
    "            response_json = response.json()['response']\n",
    "            # Extract JSON content from markdown code block if present\n",
    "            if '```json' in response_json:\n",
    "                json_str = response_json.split('```json')[1].split('```')[0].strip()\n",
    "            elif '```' in response_json:\n",
    "                json_str = response_json.split('```')[1].split('```')[0].strip()\n",
    "            else:\n",
    "                json_str = response_json.strip()\n",
    "            \n",
    "            # Remove any leading/trailing whitespace or newlines\n",
    "            json_str = json_str.strip()\n",
    "            \n",
    "            # Parse the JSON string\n",
    "            parsed_json = json.loads(json_str)\n",
    "            \n",
    "            # Ensure we have a list\n",
    "            if isinstance(parsed_json, dict):\n",
    "                parsed_json = [parsed_json]\n",
    "                \n",
    "            return parsed_json\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing LLM response: {e}\")\n",
    "            print(f\"Response was: {response.json()['response'][:500]}...\")  # Print just first 500 chars\n",
    "            return []\n",
    "    else:\n",
    "        print(f\"API request failed with status code: {response.status_code}\")\n",
    "    return []\n",
    "\n",
    "def main():\n",
    "    # Read the input file\n",
    "    with open('/workspace/rohith_llm/Extracted/KSSR_pdfminer_extracted.txt', 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    # Process text in chunks\n",
    "    chunks = chunk_text(text)\n",
    "    all_results = []\n",
    "    \n",
    "    current_part = \"I\"\n",
    "    current_section = \"\"\n",
    "    current_annexure = \"\"\n",
    "    \n",
    "    for i, chunk in enumerate(tqdm(chunks, desc=\"Processing text chunks\")):\n",
    "        # Try rule-based processing first\n",
    "        results = process_rule_based(chunk)\n",
    "        \n",
    "        # Update tracking variables if this chunk has them\n",
    "        for result in results:\n",
    "            if result[\"Part\"]:\n",
    "                current_part = result[\"Part\"]\n",
    "            if result[\"Section\"]:\n",
    "                current_section = result[\"Section\"]\n",
    "            if result[\"Annexure\"]:\n",
    "                current_annexure = result[\"Annexure\"]\n",
    "        \n",
    "        # If rule-based processing fails or finds very little, use LLM\n",
    "        if not results or len(results) < 2:\n",
    "            print(f\"Using LLM for chunk {i+1} of {len(chunks)}\")\n",
    "            llm_results = process_with_llama(chunk)\n",
    "            \n",
    "            # Update the tracking variables for any missing fields\n",
    "            for result in llm_results:\n",
    "                if not result[\"Part\"]:\n",
    "                    result[\"Part\"] = current_part\n",
    "                else:\n",
    "                    current_part = result[\"Part\"]\n",
    "                    \n",
    "                if not result[\"Section\"]:\n",
    "                    result[\"Section\"] = current_section\n",
    "                else:\n",
    "                    current_section = result[\"Section\"]\n",
    "                    \n",
    "                if not result[\"Annexure\"]:\n",
    "                    result[\"Annexure\"] = current_annexure\n",
    "                else:\n",
    "                    current_annexure = result[\"Annexure\"]\n",
    "            \n",
    "            all_results.extend(llm_results)\n",
    "        else:\n",
    "            all_results.extend(results)\n",
    "    \n",
    "    # Add debug output\n",
    "    print(f\"Total rules found: {len(all_results)}\")\n",
    "    \n",
    "    # Write results to JSON file\n",
    "    if all_results:\n",
    "        with open('Full_KSSR_extracted_rules.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(all_results, f, indent=4, ensure_ascii=False)\n",
    "        print(\"Results successfully written to Full_KSSR_extracted_rules.json\")\n",
    "    else:\n",
    "        print(\"No rules were found to write to file\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    result = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
