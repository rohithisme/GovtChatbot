{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Responses saved to /workspace/Responses.json\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import json\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_community.llms import Ollama\n",
    "import torch\n",
    "import os\n",
    "\n",
    "torch.cuda.set_device(6)\n",
    "\n",
    "class RAGChatbot:\n",
    "    def __init__(self, index_path, metadata_path, model_name='all-MiniLM-L6-v2'):\n",
    "        self.index = faiss.read_index(index_path)\n",
    "        with open(metadata_path, 'r') as f:\n",
    "            self.metadata = json.load(f)\n",
    "        self.embedder = SentenceTransformer(model_name)\n",
    "        self.llm = Ollama(model=\"llama3.3:70b-instruct-q8_0\")\n",
    "\n",
    "    def get_relevant_context(self, query, k=6):\n",
    "        query_embedding = self.embedder.encode([query])\n",
    "        distances, indices = self.index.search(query_embedding.astype('float32'), k)\n",
    "        \n",
    "        context = []\n",
    "        for idx in indices[0]:\n",
    "            meta = self.metadata[idx]\n",
    "\n",
    "            # Create a list of field-value pairs, excluding empty values\n",
    "            fields = []\n",
    "            if meta.get('Document'):\n",
    "                fields.append(f\"Document: {meta['Document']}\")\n",
    "            if meta.get('Part'):\n",
    "                fields.append(f\"Part: {meta['Part']}\")\n",
    "            if meta.get('Chapter'):\n",
    "                fields.append(f\"Chapter: {meta['Chapter']}\")\n",
    "            if meta.get('Appendix'):\n",
    "                fields.append(f\"Appendix: {meta['Appendix']}\")\n",
    "            if meta.get('Annexure'):\n",
    "                fields.append(f\"Annexure: {meta['Annexure']}\")\n",
    "            if meta.get('Section'):\n",
    "                fields.append(f\"Section: {meta['Section']}\")\n",
    "            if meta.get('Sub Section'):\n",
    "                fields.append(f\"Sub Section: {meta['Sub Section']}\")\n",
    "            if meta.get('Sub division'):\n",
    "                fields.append(f\"Sub division: {meta['Sub division']}\")\n",
    "            if meta.get('Rule no.'):\n",
    "                fields.append(f\"Rule: {meta['Rule no.']}\")\n",
    "            if meta.get('Amendment order no.'):\n",
    "                fields.append(f\"Amendment Order: {meta['Amendment order no.']}\")\n",
    "            if meta.get('Order date'):\n",
    "                fields.append(f\"Order Date: {meta['Order date']}\")\n",
    "            if meta.get('Effective date'):\n",
    "                fields.append(f\"Effective Date: {meta['Effective date']}\")\n",
    "            if meta.get('Description'):\n",
    "                fields.append(f\"Description: {meta['Description']}\")\n",
    "\n",
    "            # Join all non-empty fields with commas\n",
    "            context_string = ', '.join([f for f in fields if f])\n",
    "            context.append(context_string)\n",
    "            #print(context)\n",
    "        return context\n",
    "\n",
    "    def generate_response(self, query, context):\n",
    "        prompt = f\"\"\"You are an expert assistant in Kerala Service Rules (KSR).  \n",
    "Follow these guidelines for your responses:\n",
    "1. Use simple, everyday language that anyone can understand\n",
    "2. Organize your answer in clear paragraphs with one main idea per paragraph\n",
    "3. Start with the most important information first\n",
    "4. Include proper references (document, part, chapter, rule number, etc.) when available\n",
    "5. Clearly state if the answer cannot be found in the provided rules\n",
    "6. Avoid technical jargon unless absolutely necessary, and explain any technical terms you must use\n",
    "7. Use short sentences and simple sentence structure\n",
    "8. DO NOT fabricate information. If the answer is not found in the rules, explicitly state so.\n",
    "\n",
    "        Relevant Rules:\n",
    "        {' '.join(context)}\n",
    "\n",
    "        Question: {query}\n",
    "        Answer:\"\"\"\n",
    "        \n",
    "        return self.llm.invoke(prompt)\n",
    "\n",
    "    def process_questions(self, question_file, output_file):\n",
    "        with open(question_file, 'r') as f:\n",
    "            questions = [line.strip() for line in f if line.strip()]\n",
    "        \n",
    "        results = []\n",
    "        for question in questions:\n",
    "            context = self.get_relevant_context(question)\n",
    "            response = self.generate_response(question, context)\n",
    "            results.append({\"instruction\": question, \"input\": \"\", \"output\": response})\n",
    "        \n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(results, f, indent=4, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"Responses saved to {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chatbot = RAGChatbot(\n",
    "        '/workspace/Extracted/Structured/Summary/Vector_DB/embeddings.faiss',\n",
    "        '/workspace/Extracted/Structured/Summary/Vector_DB/metadata.json'\n",
    "    )\n",
    "    chatbot.process_questions('/workspace/Questions.txt', '/workspace/Responses.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (2.4.1+cu118)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.46.3)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.8/dist-packages (3.1.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.8/dist-packages (from torch) (2.6.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.8/dist-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.8/dist-packages (from torch) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.8/dist-packages (from torch) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.8/dist-packages (from torch) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /usr/local/lib/python3.8/dist-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.8/dist-packages (from torch) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.8/dist-packages (from torch) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.8/dist-packages (from torch) (10.3.0.86)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.8/dist-packages (from torch) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.8/dist-packages (from torch) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.20.5 in /usr/local/lib/python3.8/dist-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.8/dist-packages (from torch) (11.8.86)\n",
      "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.8/dist-packages (from torch) (3.0.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.29.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.22.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets) (1.4.4)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.8/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets) (3.10.11)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.15.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.9.24)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2022.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.46.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.29.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.22.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.9.24)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade torch transformers datasets\n",
    "!pip install --upgrade transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.trainer because of the following error (look up to see its traceback):\nNo module named 'torch._six'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/utils/import_utils.py:1778\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1777\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1778\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/lib/python3.8/importlib/__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    126\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1014\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:991\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:975\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:671\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:848\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:219\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer.py:193\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_apex_available():\n\u001b[0;32m--> 193\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mapex\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m amp\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_datasets_available():\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/apex/__init__.py:15\u001b[0m\n\u001b[1;32m     13\u001b[0m     __all__\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m amp\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fp16_utils\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/apex/amp/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mamp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m init, half_function, float_function, promote_function,\\\n\u001b[1;32m      2\u001b[0m     register_half_function, register_float_function, register_promote_function\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhandle\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m scale_loss, disable_casts\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/apex/amp/amp.py:10\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_amp_state\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _amp_state\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfrontend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     13\u001b[0m _DECORATOR_HANDLE \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/apex/amp/frontend.py:5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_initialize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _initialize\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_amp_state\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _amp_state, warn_or_err, maybe_print\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/apex/amp/_initialize.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_six\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m string_classes\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfunctools\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch._six'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Ollama\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/sentence_transformers/__init__.py:18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mSentenceTransformer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msimilarity_functions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SimilarityFunction\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformerTrainer\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining_args\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformerTrainingArguments\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# If codecarbon is installed and the log level is not defined,\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# automatically overwrite the default to \"error\"\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/sentence_transformers/trainer.py:12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BatchSampler, ConcatDataset, DataLoader, SubsetRandomSampler\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EvalPrediction, PreTrainedTokenizerBase, Trainer, TrainerCallback\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_collator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataCollator\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegrations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WandbCallback\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1039\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/utils/import_utils.py:1766\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1764\u001b[0m     value \u001b[38;5;241m=\u001b[39m Placeholder\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m-> 1766\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1767\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[1;32m   1768\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/utils/import_utils.py:1780\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1778\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1780\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1781\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1783\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.trainer because of the following error (look up to see its traceback):\nNo module named 'torch._six'"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import json\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_community.llms import Ollama\n",
    "import torch\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "torch.cuda.set_device(6)\n",
    "\n",
    "class RAGChatbot:\n",
    "    def __init__(self, index_path, metadata_path, model_name='all-MiniLM-L6-v2', checkpoint_dir='checkpoints'):\n",
    "        self.index = faiss.read_index(index_path)\n",
    "        with open(metadata_path, 'r') as f:\n",
    "            self.metadata = json.load(f)\n",
    "        self.embedder = SentenceTransformer(model_name)\n",
    "        self.llm = Ollama(model=\"llama3.3:70b-instruct-q8_0\")\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        \n",
    "        # Create checkpoint directory if it doesn't exist\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "\n",
    "    def get_relevant_context(self, query, k=6):\n",
    "        query_embedding = self.embedder.encode([query])\n",
    "        distances, indices = self.index.search(query_embedding.astype('float32'), k)\n",
    "        \n",
    "        context = []\n",
    "        for idx in indices[0]:\n",
    "            meta = self.metadata[idx]\n",
    "\n",
    "            # Create a list of field-value pairs, excluding empty values\n",
    "            fields = []\n",
    "            if meta.get('Document'):\n",
    "                fields.append(f\"Document: {meta['Document']}\")\n",
    "            if meta.get('Part'):\n",
    "                fields.append(f\"Part: {meta['Part']}\")\n",
    "            if meta.get('Chapter'):\n",
    "                fields.append(f\"Chapter: {meta['Chapter']}\")\n",
    "            if meta.get('Appendix'):\n",
    "                fields.append(f\"Appendix: {meta['Appendix']}\")\n",
    "            if meta.get('Annexure'):\n",
    "                fields.append(f\"Annexure: {meta['Annexure']}\")\n",
    "            if meta.get('Section'):\n",
    "                fields.append(f\"Section: {meta['Section']}\")\n",
    "            if meta.get('Sub Section'):\n",
    "                fields.append(f\"Sub Section: {meta['Sub Section']}\")\n",
    "            if meta.get('Sub division'):\n",
    "                fields.append(f\"Sub division: {meta['Sub division']}\")\n",
    "            if meta.get('Rule no.'):\n",
    "                fields.append(f\"Rule: {meta['Rule no.']}\")\n",
    "            if meta.get('Amendment order no.'):\n",
    "                fields.append(f\"Amendment Order: {meta['Amendment order no.']}\")\n",
    "            if meta.get('Order date'):\n",
    "                fields.append(f\"Order Date: {meta['Order date']}\")\n",
    "            if meta.get('Effective date'):\n",
    "                fields.append(f\"Effective Date: {meta['Effective date']}\")\n",
    "            if meta.get('Description'):\n",
    "                fields.append(f\"Description: {meta['Description']}\")\n",
    "\n",
    "            # Join all non-empty fields with commas\n",
    "            context_string = ', '.join([f for f in fields if f])\n",
    "            context.append(context_string)\n",
    "        return context\n",
    "\n",
    "    def generate_response(self, query, context):\n",
    "        prompt = f\"\"\"You are an expert assistant in Kerala Service Rules (KSR).  \n",
    "Follow these guidelines for your responses:\n",
    "1. Use simple, everyday language that anyone can understand\n",
    "2. Organize your answer in clear paragraphs with one main idea per paragraph\n",
    "3. Start with the most important information first\n",
    "4. Include proper references (document, part, chapter, rule number, etc.) when available\n",
    "5. Clearly state if the answer cannot be found in the provided rules\n",
    "6. Avoid technical jargon unless absolutely necessary, and explain any technical terms you must use\n",
    "7. Use short sentences and simple sentence structure\n",
    "8. DO NOT fabricate information. If the answer is not found in the rules, explicitly state so.\n",
    "\n",
    "        Relevant Rules:\n",
    "        {' '.join(context)}\n",
    "\n",
    "        Question: {query}\n",
    "        Answer:\"\"\"\n",
    "        \n",
    "        return self.llm.invoke(prompt)\n",
    "    \n",
    "    def save_checkpoint(self, results, checkpoint_name):\n",
    "        \"\"\"Save progress checkpoint.\"\"\"\n",
    "        checkpoint_path = os.path.join(self.checkpoint_dir, checkpoint_name)\n",
    "        with open(checkpoint_path, 'w') as f:\n",
    "            json.dump(results, f, indent=4, ensure_ascii=False)\n",
    "        print(f\"\\nCheckpoint saved to {checkpoint_path}\")\n",
    "    \n",
    "    def load_checkpoint(self, checkpoint_name):\n",
    "        \"\"\"Load progress from checkpoint.\"\"\"\n",
    "        checkpoint_path = os.path.join(self.checkpoint_dir, checkpoint_name)\n",
    "        if os.path.exists(checkpoint_path):\n",
    "            with open(checkpoint_path, 'r') as f:\n",
    "                return json.load(f)\n",
    "        return None\n",
    "\n",
    "    def process_questions(self, question_file, output_file, checkpoint_size=20):\n",
    "        # Load questions\n",
    "        with open(question_file, 'r') as f:\n",
    "            questions = [line.strip() for line in f if line.strip()]\n",
    "        \n",
    "        # Check for most recent checkpoint\n",
    "        checkpoints = [f for f in os.listdir(self.checkpoint_dir) if f.startswith('checkpoint_') and f.endswith('.json')]\n",
    "        most_recent_checkpoint = None\n",
    "        start_idx = 0\n",
    "        results = []\n",
    "        \n",
    "        if checkpoints:\n",
    "            # Sort checkpoints by their timestamp\n",
    "            checkpoints.sort(key=lambda x: int(x.split('_')[1].split('.')[0]))\n",
    "            most_recent_checkpoint = checkpoints[-1]\n",
    "            loaded_results = self.load_checkpoint(most_recent_checkpoint)\n",
    "            \n",
    "            if loaded_results:\n",
    "                results = loaded_results\n",
    "                start_idx = len(results)\n",
    "                print(f\"Resuming from checkpoint with {start_idx} questions already processed\")\n",
    "        \n",
    "        # Process remaining questions with progress bar\n",
    "        if start_idx < len(questions):\n",
    "            print(f\"Processing questions {start_idx+1} to {len(questions)}\")\n",
    "            \n",
    "            for i in tqdm(range(start_idx, len(questions)), desc=\"Processing questions\"):\n",
    "                question = questions[i]\n",
    "                try:\n",
    "                    context = self.get_relevant_context(question)\n",
    "                    response = self.generate_response(question, context)\n",
    "                    results.append({\"instruction\": question, \"input\": \"\", \"output\": response})\n",
    "                    \n",
    "                    # Create checkpoint after every checkpoint_size questions\n",
    "                    if (i + 1) % checkpoint_size == 0 or i == len(questions) - 1:\n",
    "                        checkpoint_name = f\"checkpoint_{len(results)}.json\"\n",
    "                        self.save_checkpoint(results, checkpoint_name)\n",
    "                        \n",
    "                        # Remove older checkpoints, keeping only the 3 most recent\n",
    "                        self._cleanup_old_checkpoints(3)\n",
    "                    \n",
    "                    # Small delay to prevent overwhelming the system\n",
    "                    time.sleep(0.1)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"\\nError processing question {i+1}: {e}\")\n",
    "                    # Save checkpoint on error\n",
    "                    checkpoint_name = f\"checkpoint_{len(results)}_error.json\"\n",
    "                    self.save_checkpoint(results, checkpoint_name)\n",
    "                    raise\n",
    "        \n",
    "        # Save final output\n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(results, f, indent=4, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"\\nAll responses saved to {output_file}\")\n",
    "    \n",
    "    def _cleanup_old_checkpoints(self, keep_count=3):\n",
    "        \"\"\"Keep only the most recent checkpoints.\"\"\"\n",
    "        checkpoints = [f for f in os.listdir(self.checkpoint_dir) if f.startswith('checkpoint_') and f.endswith('.json')]\n",
    "        \n",
    "        # Sort checkpoints by their timestamp (numeric part)\n",
    "        checkpoints.sort(key=lambda x: int(x.split('_')[1].split('.')[0]))\n",
    "        \n",
    "        # Remove older checkpoints, leaving the most recent ones\n",
    "        if len(checkpoints) > keep_count:\n",
    "            for old_checkpoint in checkpoints[:-keep_count]:\n",
    "                try:\n",
    "                    os.remove(os.path.join(self.checkpoint_dir, old_checkpoint))\n",
    "                except Exception as e:\n",
    "                    print(f\"Error removing old checkpoint {old_checkpoint}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chatbot = RAGChatbot(\n",
    "        '/workspace/Extracted/Structured/Summary/Vector_DB/embeddings.faiss',\n",
    "        '/workspace/Extracted/Structured/Summary/Vector_DB/metadata.json'\n",
    "    )\n",
    "    chatbot.process_questions('/workspace/Questions.txt', '/workspace/Responses.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
