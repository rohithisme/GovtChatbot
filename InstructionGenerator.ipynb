{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19ff517b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting pypdf2\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: typing_extensions>=3.10.0.0 in /usr/local/lib/python3.8/dist-packages (from pypdf2) (4.12.2)\n",
      "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "Installing collected packages: pypdf2\n",
      "Successfully installed pypdf2-3.0.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pypdf2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e309ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating dataset:  34%|█████████████████████████████████████▍                                                                        | 2139/6293 [11:47:38<21:16:10, 18.43s/it]"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import ollama\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "# Load structured data from JSON\n",
    "with open(\"/workspace/rohith_llm/Extracted/Structured/Summary/Combined_Metadata.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Define Alpaca dataset format\n",
    "alpaca_dataset = []\n",
    "\n",
    "# Function to generate contextually relevant questions using the LLM\n",
    "def generate_contextual_question(rule):\n",
    "    # Prepare a prompt that asks the LLM to analyze the rule and generate a contextual question\n",
    "    rule_content = f\"Rule Description: {rule.get('Description', '')}\\nRule Content: {rule.get('Content', '')}\"\n",
    "    \n",
    "    # Ask the LLM to analyze the rule and generate an appropriate question\n",
    "    analysis_response = ollama.chat(\n",
    "        model=\"llama3.3:70b-instruct-q8_0\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\", \n",
    "                \"content\": \"You are an expert in Kerala government rules. Given a government rule, generate a single, specific question that would be relevant for someone wanting to learn about this rule. Your response should ONLY include the question itself, with no additional text or explanations.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": f\"Here is a Kerala government service rule. Generate only a specific, contextually relevant question about this rule:\\n\\n{rule_content}\"\n",
    "            }\n",
    "        ]\n",
    "    )[\"message\"][\"content\"]\n",
    "    \n",
    "    # Clean up the response to ensure we only get the question\n",
    "    question = analysis_response.strip()\n",
    "    \n",
    "    # Remove any explanatory text that might be included\n",
    "    if \":\" in question and not question.startswith('\"'):\n",
    "        question = question.split(\":\", 1)[1].strip()\n",
    "    \n",
    "    # Remove quotes if the LLM added them\n",
    "    question = re.sub(r'^[\"\\'](.*)[\"\\']$', r'\\1', question)\n",
    "    \n",
    "    return question\n",
    "\n",
    "# Process each rule to generate instruction-response pairs\n",
    "for rule in tqdm(data, desc=\"Generating dataset\"):\n",
    "    if rule.get(\"Description\"):  # Ensure description is present\n",
    "        # First, use the LLM to generate a contextually relevant question\n",
    "        instruction = generate_contextual_question(rule)\n",
    "        \n",
    "        # Then, use the LLM to generate a detailed answer to that question\n",
    "        # Pass the rule content along with the question to ensure the LLM has context\n",
    "        rule_content = f\"Rule Description: {rule.get('Description', '')}\\nRule Content: {rule.get('Content', '')}\"\n",
    "        \n",
    "        response = ollama.chat(\n",
    "            model=\"llama3.3:70b-instruct-q8_0\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\", \n",
    "                    \"content\": \"You are an expert in Kerala government rules. Provide a detailed and structured response based on the given rule.  Include proper references (document, part, chapter, rule number, etc.) when available. Use simple, everyday language that anyone can understand \"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": f\"Based on the following Kerala government rule:\\n\\n{rule_content}\\n\\nPlease answer this question: {instruction}\"\n",
    "                }\n",
    "            ]\n",
    "        )[\"message\"][\"content\"]\n",
    "        \n",
    "        alpaca_dataset.append({\n",
    "            \"instruction\": instruction,\n",
    "            \"input\": \"\",\n",
    "            \"output\": response\n",
    "        })\n",
    "\n",
    "    # Stop when we reach 10 entries for testing\n",
    "    if len(alpaca_dataset) >= 10000:\n",
    "        break\n",
    "\n",
    "# Save dataset to JSON\n",
    "with open(\"kerala_llm_instruction_dataset.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(alpaca_dataset, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Dataset generation complete. Saved to kerala_llm_instruction_dataset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774e3e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating dataset:   0%|                                                                                                                         | 0/10000 [00:24<?, ?it/s]\u001b[A\n",
      "\n",
      "Generating dataset:   0%|                                                                                                            | 1/10000 [01:44<290:40:59, 104.66s/it]\u001b[A\n",
      "Generating dataset:   0%|                                                                                                             | 2/10000 [01:51<131:18:33, 47.28s/it]\u001b[A\n",
      "Generating dataset:   0%|                                                                                                             | 3/10000 [02:23<112:09:18, 40.39s/it]\u001b[A\n",
      "Generating dataset:   0%|                                                                                                              | 4/10000 [02:46<92:09:31, 33.19s/it]\u001b[A\n",
      "Generating dataset:   0%|                                                                                                              | 5/10000 [03:14<87:29:42, 31.51s/it]\u001b[A\n",
      "Generating dataset:   0%|                                                                                                              | 6/10000 [03:44<86:08:59, 31.03s/it]\u001b[A\n",
      "Generating dataset:   0%|                                                                                                              | 7/10000 [04:20<90:00:39, 32.43s/it]\u001b[A\n",
      "Generating dataset:   0%|                                                                                                              | 8/10000 [04:40<79:21:10, 28.59s/it]\u001b[A\n",
      "Generating dataset:   0%|                                                                                                              | 9/10000 [04:51<64:10:36, 23.12s/it]\u001b[A\n",
      "Generating dataset:   0%|                                                                                                             | 10/10000 [05:18<67:09:50, 24.20s/it]\u001b[A\n",
      "Generating dataset:   0%|                                                                                                             | 11/10000 [05:46<70:28:48, 25.40s/it]\u001b[A\n",
      "Generating dataset:   0%|▏                                                                                                            | 12/10000 [05:58<59:27:40, 21.43s/it]\u001b[A\n",
      "Generating dataset:   0%|▏                                                                                                            | 13/10000 [06:24<63:24:43, 22.86s/it]\u001b[A\n",
      "Generating dataset:   0%|▏                                                                                                            | 14/10000 [06:38<56:06:58, 20.23s/it]\u001b[A\n",
      "Generating dataset:   0%|▏                                                                                                            | 15/10000 [06:53<51:22:09, 18.52s/it]\u001b[A\n",
      "Generating dataset:   0%|▏                                                                                                            | 16/10000 [07:24<61:54:28, 22.32s/it]\u001b[A\n",
      "Generating dataset:   0%|▏                                                                                                            | 17/10000 [07:48<63:14:18, 22.80s/it]\u001b[A\n",
      "Generating dataset:   0%|▏                                                                                                            | 18/10000 [08:08<60:32:23, 21.83s/it]\u001b[A\n",
      "Generating dataset:   0%|▏                                                                                                            | 19/10000 [08:28<59:43:25, 21.54s/it]\u001b[A\n",
      "Generating dataset:   0%|▏                                                                                                            | 20/10000 [08:54<63:10:30, 22.79s/it]\u001b[A"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import ollama\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import random\n",
    "import torch\n",
    "import os\n",
    "\n",
    "\n",
    "torch.cuda.set_device(7)\n",
    "\n",
    "# Load structured data from JSON\n",
    "with open(\"/workspace/rohith_llm/Extracted/Structured/Summary/Combined_Metadata.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Define Alpaca dataset format\n",
    "alpaca_dataset = []\n",
    "\n",
    "# Function to determine how many questions to generate for a rule\n",
    "def determine_question_count(rule):\n",
    "    # Check if the rule has substantial content\n",
    "    content_length = len(rule.get('Content', ''))\n",
    "    description_length = len(rule.get('Description', ''))\n",
    "    \n",
    "    # Rules with more content can generate more questions\n",
    "    if content_length > 1000 or description_length > 200:\n",
    "        return random.randint(2, 4)  # Generate 2-4 questions for substantial rules\n",
    "    else:\n",
    "        return 1  # Generate at least 1 question for each rule\n",
    "\n",
    "# Function to generate multiple contextually relevant questions using the LLM\n",
    "def generate_contextual_questions(rule, count):\n",
    "    # Prepare a prompt that asks the LLM to analyze the rule and generate multiple contextual questions\n",
    "    rule_content = f\"Rule Description: {rule.get('Description', '')}\\nRule Content: {rule.get('Content', '')}\"\n",
    "    \n",
    "    # Ask the LLM to generate multiple different questions\n",
    "    analysis_response = ollama.chat(\n",
    "        model=\"llama3.3:70b-instruct-q8_0\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\", \n",
    "                \"content\": \"You are an expert in Kerala government rules. Given a government rule, generate multiple specific, diverse questions that would be relevant for someone wanting to learn about this rule. Generate exactly the number of questions requested. Format each question on a separate line with a number followed by a period (e.g., '1.', '2.'). Your questions should cover different aspects of the rule.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": f\"Here is a Kerala government service rule. Generate {count} specific, contextually relevant questions about this rule:\\n\\n{rule_content}\"\n",
    "            }\n",
    "        ]\n",
    "    )[\"message\"][\"content\"]\n",
    "    \n",
    "    # Parse the numbered questions\n",
    "    questions = []\n",
    "    lines = analysis_response.strip().split(\"\\n\")\n",
    "    for line in lines:\n",
    "        # Look for numbered lines like \"1. Question\" or \"1) Question\"\n",
    "        match = re.match(r'^\\d+[\\.\\)]\\s+(.*)', line)\n",
    "        if match:\n",
    "            question = match.group(1).strip()\n",
    "            # Remove quotes if the LLM added them\n",
    "            question = re.sub(r'^[\"\\'](.*)[\"\\']$', r'\\1', question)\n",
    "            questions.append(question)\n",
    "    \n",
    "    # If parsing failed, try to split by newlines and take the first 'count' non-empty lines\n",
    "    if len(questions) < count:\n",
    "        questions = [line.strip() for line in lines if line.strip()]\n",
    "        questions = questions[:count]\n",
    "    \n",
    "    # If we still don't have enough questions, generate a default one\n",
    "    if len(questions) < count:\n",
    "        questions.append(f\"What are the key provisions in the rule about {rule.get('Description', 'this Kerala government regulation')}?\")\n",
    "    \n",
    "    return questions[:count]  # Return only the requested number of questions\n",
    "\n",
    "# Function to generate a detailed answer to a question\n",
    "def generate_response(rule, question):\n",
    "    rule_content = f\"Rule Description: {rule.get('Description', '')}\\nRule Content: {rule.get('Content', '')}\"\n",
    "    \n",
    "    response = ollama.chat(\n",
    "        model=\"llama3.3:70b-instruct-q8_0\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\", \n",
    "                \"content\": \"You are an expert in Kerala government rules. Provide a detailed and structured response based on the given rule. Include proper references (document, part, chapter, rule number, etc.) when available. Use simple, everyday language that anyone can understand.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": f\"Based on the following Kerala government rule:\\n\\n{rule_content}\\n\\nPlease answer this question: {question}\"\n",
    "            }\n",
    "        ]\n",
    "    )[\"message\"][\"content\"]\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Process each rule to generate instruction-response pairs\n",
    "target_count = 10000\n",
    "pbar = tqdm(total=target_count, desc=\"Generating dataset\")\n",
    "\n",
    "# First pass: process all rules at least once\n",
    "for rule in data:\n",
    "    if not rule.get(\"Description\"):\n",
    "        continue  # Skip rules without description\n",
    "        \n",
    "    # Determine how many questions to generate for this rule\n",
    "    question_count = determine_question_count(rule)\n",
    "    \n",
    "    # Generate multiple questions for this rule\n",
    "    questions = generate_contextual_questions(rule, question_count)\n",
    "    \n",
    "    # Process each question\n",
    "    for question in questions:\n",
    "        response = generate_response(rule, question)\n",
    "        \n",
    "        alpaca_dataset.append({\n",
    "            \"instruction\": question,\n",
    "            \"input\": \"\",\n",
    "            \"output\": response\n",
    "        })\n",
    "        \n",
    "        pbar.update(1)\n",
    "        \n",
    "        # Check if we've reached our target\n",
    "        if len(alpaca_dataset) >= target_count:\n",
    "            break\n",
    "    \n",
    "    # Break the outer loop if we've reached our target\n",
    "    if len(alpaca_dataset) >= target_count:\n",
    "        break\n",
    "\n",
    "# If we still don't have enough entries, do a second pass on rules with substantial content\n",
    "if len(alpaca_dataset) < target_count:\n",
    "    # Sort rules by content length to prioritize substantial rules\n",
    "    sorted_rules = sorted(data, key=lambda x: len(x.get('Content', '')), reverse=True)\n",
    "    \n",
    "    for rule in sorted_rules:\n",
    "        if not rule.get(\"Description\"):\n",
    "            continue  # Skip rules without description\n",
    "            \n",
    "        # Generate additional questions beyond what we did in the first pass\n",
    "        additional_questions = generate_contextual_questions(rule, 2)  # Generate 2 more questions\n",
    "        \n",
    "        for question in additional_questions:\n",
    "            # Check if this question is too similar to ones we already asked for this rule\n",
    "            existing_questions = [item[\"instruction\"] for item in alpaca_dataset]\n",
    "            if any(similar(question, eq) for eq in existing_questions):\n",
    "                continue  # Skip similar questions\n",
    "                \n",
    "            response = generate_response(rule, question)\n",
    "            \n",
    "            alpaca_dataset.append({\n",
    "                \"instruction\": question,\n",
    "                \"input\": \"\",\n",
    "                \"output\": response\n",
    "            })\n",
    "            \n",
    "            pbar.update(1)\n",
    "            \n",
    "            # Check if we've reached our target\n",
    "            if len(alpaca_dataset) >= target_count:\n",
    "                break\n",
    "        \n",
    "        # Break the outer loop if we've reached our target\n",
    "        if len(alpaca_dataset) >= target_count:\n",
    "            break\n",
    "\n",
    "# Simple function to check if two questions are similar\n",
    "def similar(q1, q2):\n",
    "    # Very basic similarity check - can be improved\n",
    "    common_words = set(q1.lower().split()) & set(q2.lower().split())\n",
    "    return len(common_words) > 3  # If they share more than 3 words, consider them similar\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "# Save dataset to JSON\n",
    "with open(\"kerala_llm_instruction_dataset.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(alpaca_dataset, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Dataset generation complete. Generated {len(alpaca_dataset)} instruction-response pairs. Saved to kerala_llm_instruction_dataset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05fc1874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading latest checkpoint: kerala_dataset_checkpoint_8980.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating dataset:  90%|██████████████████████████████████████████████████████████████████▌       | 9000/10000 [09:10<7:10:09, 25.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved: 9000/10000 (90.00%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating dataset:  90%|██████████████████████████████████████████████████████████████████▋       | 9020/10000 [16:49<6:00:58, 22.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved: 9020/10000 (90.20%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating dataset:  90%|██████████████████████████████████████████████████████████████████▉       | 9040/10000 [24:11<5:22:35, 20.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved: 9040/10000 (90.40%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating dataset:  90%|██████████████████████████████████████████████████████████████████▉       | 9042/10000 [25:09<6:28:43, 24.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved: 9042/10000 (90.42%)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 167\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m# Process each question\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m question \u001b[38;5;129;01min\u001b[39;00m questions:\n\u001b[0;32m--> 167\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m     alpaca_dataset\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m    170\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstruction\u001b[39m\u001b[38;5;124m\"\u001b[39m: question,\n\u001b[1;32m    171\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    172\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m: response\n\u001b[1;32m    173\u001b[0m     })\n\u001b[1;32m    175\u001b[0m     current_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(alpaca_dataset)\n",
      "Cell \u001b[0;32mIn [1], line 129\u001b[0m, in \u001b[0;36mgenerate_response\u001b[0;34m(rule, question)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_response\u001b[39m(rule, question):\n\u001b[1;32m    127\u001b[0m     rule_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRule Description: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrule\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDescription\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mRule Content: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrule\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mContent\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 129\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mollama\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mllama3.3:70b-instruct-q8_0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYou are an expert in Kerala government rules. Provide a detailed and structured response based on the given rule. Include proper references (document, part, chapter, rule number, etc.) when available. Use simple, everyday language that anyone can understand.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    135\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBased on the following Kerala government rule:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mrule_content\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mPlease answer this question: \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mquestion\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    139\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ollama/_client.py:333\u001b[0m, in \u001b[0;36mClient.chat\u001b[0;34m(self, model, messages, tools, stream, format, options, keep_alive)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchat\u001b[39m(\n\u001b[1;32m    290\u001b[0m   \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    291\u001b[0m   model: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    298\u001b[0m   keep_alive: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    299\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[ChatResponse, Iterator[ChatResponse]]:\n\u001b[1;32m    300\u001b[0m   \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;124;03m  Create a chat response using the requested model.\u001b[39;00m\n\u001b[1;32m    302\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;124;03m  Returns `ChatResponse` if `stream` is `False`, otherwise returns a `ChatResponse` generator.\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 333\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[43mChatResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/api/chat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatRequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_copy_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtool\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtool\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_copy_tools\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m      \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m      \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m      \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_dump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexclude_none\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ollama/_client.py:178\u001b[0m, in \u001b[0;36mClient._request\u001b[0;34m(self, cls, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpart)\n\u001b[1;32m    176\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[0;32m--> 178\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mjson())\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ollama/_client.py:118\u001b[0m, in \u001b[0;36mClient._request_raw\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_request_raw\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    117\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 118\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m     r\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/httpx/_client.py:825\u001b[0m, in \u001b[0;36mClient.request\u001b[0;34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[0m\n\u001b[1;32m    810\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    812\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_request(\n\u001b[1;32m    813\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[1;32m    814\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    823\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mextensions,\n\u001b[1;32m    824\u001b[0m )\n\u001b[0;32m--> 825\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/httpx/_client.py:914\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    910\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_timeout(request)\n\u001b[1;32m    912\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 914\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    921\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/httpx/_client.py:942\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    939\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 942\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    947\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    948\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/httpx/_client.py:979\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    977\u001b[0m     hook(request)\n\u001b[0;32m--> 979\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    981\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/httpx/_client.py:1014\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1009\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1010\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1011\u001b[0m     )\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1014\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1018\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/httpx/_transports/default.py:250\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    237\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    238\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    239\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    247\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    248\u001b[0m )\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 250\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    255\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    256\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    257\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[1;32m    258\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    259\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/httpcore/_sync/connection_pool.py:256\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    253\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/httpcore/_sync/connection_pool.py:236\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    232\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 236\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/httpcore/_sync/connection.py:103\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/httpcore/_sync/http11.py:136\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[0;32m--> 136\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/httpcore/_sync/http11.py:106\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m     99\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    100\u001b[0m     (\n\u001b[1;32m    101\u001b[0m         http_version,\n\u001b[1;32m    102\u001b[0m         status,\n\u001b[1;32m    103\u001b[0m         reason_phrase,\n\u001b[1;32m    104\u001b[0m         headers,\n\u001b[1;32m    105\u001b[0m         trailing_data,\n\u001b[0;32m--> 106\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    108\u001b[0m         http_version,\n\u001b[1;32m    109\u001b[0m         status,\n\u001b[1;32m    110\u001b[0m         reason_phrase,\n\u001b[1;32m    111\u001b[0m         headers,\n\u001b[1;32m    112\u001b[0m     )\n\u001b[1;32m    114\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/httpcore/_sync/http11.py:177\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    174\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 177\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[1;32m    179\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/httpcore/_sync/http11.py:217\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    214\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 217\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/httpcore/_backends/sync.py:128\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "import ollama\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import random\n",
    "import torch\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "#torch.cuda.set_device(5)\n",
    "\n",
    "# Create a checkpoints directory if it doesn't exist\n",
    "checkpoint_dir = \"checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Function to load existing checkpoint if available\n",
    "def load_latest_checkpoint():\n",
    "    checkpoint_files = [f for f in os.listdir(checkpoint_dir) if f.startswith(\"kerala_dataset_checkpoint_\") and f.endswith(\".json\")]\n",
    "    \n",
    "    if not checkpoint_files:\n",
    "        return [], 0  # Return empty dataset and count 0 if no checkpoints found\n",
    "    \n",
    "    # Sort checkpoint files by their numbers\n",
    "    checkpoint_files.sort(key=lambda x: int(x.split(\"_\")[-1].split(\".\")[0]))\n",
    "    latest_checkpoint = checkpoint_files[-1]\n",
    "    \n",
    "    print(f\"Loading latest checkpoint: {latest_checkpoint}\")\n",
    "    \n",
    "    with open(os.path.join(checkpoint_dir, latest_checkpoint), \"r\", encoding=\"utf-8\") as f:\n",
    "        dataset = json.load(f)\n",
    "    \n",
    "    count = int(latest_checkpoint.split(\"_\")[-1].split(\".\")[0])\n",
    "    \n",
    "    return dataset, count\n",
    "\n",
    "# Function to save checkpoint\n",
    "def save_checkpoint(dataset, count):\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, f\"kerala_dataset_checkpoint_{count}.json\")\n",
    "    \n",
    "    with open(checkpoint_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(dataset, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    # Also save a status file with timestamp for quick progress checks\n",
    "    status_path = os.path.join(checkpoint_dir, \"current_status.txt\")\n",
    "    with open(status_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        f.write(f\"Last update: {timestamp}\\n\")\n",
    "        f.write(f\"Progress: {count}/{target_count} ({count/target_count*100:.2f}%)\\n\")\n",
    "    \n",
    "    print(f\"Checkpoint saved: {count}/{target_count} ({count/target_count*100:.2f}%)\")\n",
    "\n",
    "# Load structured data from JSON\n",
    "with open(\"/workspace/rohith_llm/Extracted/Structured/Summary/Combined_Metadata.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Define Alpaca dataset format and target count\n",
    "target_count = 10000\n",
    "\n",
    "# Load any existing checkpoint\n",
    "alpaca_dataset, current_count = load_latest_checkpoint()\n",
    "\n",
    "# Simple function to check if two questions are similar\n",
    "def similar(q1, q2):\n",
    "    # Very basic similarity check - can be improved\n",
    "    common_words = set(q1.lower().split()) & set(q2.lower().split())\n",
    "    return len(common_words) > 3  # If they share more than 3 words, consider them similar\n",
    "\n",
    "# Function to determine how many questions to generate for a rule\n",
    "def determine_question_count(rule):\n",
    "    # Check if the rule has substantial content\n",
    "    content_length = len(rule.get('Content', ''))\n",
    "    description_length = len(rule.get('Description', ''))\n",
    "    \n",
    "    # Rules with more content can generate more questions\n",
    "    if content_length > 1000 or description_length > 200:\n",
    "        return random.randint(2, 4)  # Generate 2-4 questions for substantial rules\n",
    "    else:\n",
    "        return 1  # Generate at least 1 question for each rule\n",
    "\n",
    "# Function to generate multiple contextually relevant questions using the LLM\n",
    "def generate_contextual_questions(rule, count):\n",
    "    # Prepare a prompt that asks the LLM to analyze the rule and generate multiple contextual questions\n",
    "    rule_content = f\"Rule Description: {rule.get('Description', '')}\\nRule Content: {rule.get('Content', '')}\"\n",
    "    \n",
    "    # Ask the LLM to generate multiple different questions\n",
    "    analysis_response = ollama.chat(\n",
    "        model=\"llama3.3:70b-instruct-q8_0\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\", \n",
    "                \"content\": \"You are an expert in Kerala government rules. Given a government rule, generate multiple specific, diverse questions that would be relevant for someone wanting to learn about this rule. Generate exactly the number of questions requested. Format each question on a separate line with a number followed by a period (e.g., '1.', '2.'). Your questions should cover different aspects of the rule.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": f\"Here is a Kerala government service rule. Generate {count} specific, contextually relevant questions about this rule:\\n\\n{rule_content}\"\n",
    "            }\n",
    "        ]\n",
    "    )[\"message\"][\"content\"]\n",
    "    \n",
    "    # Parse the numbered questions\n",
    "    questions = []\n",
    "    lines = analysis_response.strip().split(\"\\n\")\n",
    "    for line in lines:\n",
    "        # Look for numbered lines like \"1. Question\" or \"1) Question\"\n",
    "        match = re.match(r'^\\d+[\\.\\)]\\s+(.*)', line)\n",
    "        if match:\n",
    "            question = match.group(1).strip()\n",
    "            # Remove quotes if the LLM added them\n",
    "            question = re.sub(r'^[\"\\'](.*)[\"\\']$', r'\\1', question)\n",
    "            questions.append(question)\n",
    "    \n",
    "    # If parsing failed, try to split by newlines and take the first 'count' non-empty lines\n",
    "    if len(questions) < count:\n",
    "        questions = [line.strip() for line in lines if line.strip()]\n",
    "        questions = questions[:count]\n",
    "    \n",
    "    # If we still don't have enough questions, generate a default one\n",
    "    if len(questions) < count:\n",
    "        questions.append(f\"What are the key provisions in the rule about {rule.get('Description', 'this Kerala government regulation')}?\")\n",
    "    \n",
    "    return questions[:count]  # Return only the requested number of questions\n",
    "\n",
    "# Function to generate a detailed answer to a question\n",
    "def generate_response(rule, question):\n",
    "    rule_content = f\"Rule Description: {rule.get('Description', '')}\\nRule Content: {rule.get('Content', '')}\"\n",
    "    \n",
    "    response = ollama.chat(\n",
    "        model=\"llama3.3:70b-instruct-q8_0\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\", \n",
    "                \"content\": \"You are an expert in Kerala government rules. Provide a detailed and structured response based on the given rule. Include proper references (document, part, chapter, rule number, etc.) when available. Use simple, everyday language that anyone can understand.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": f\"Based on the following Kerala government rule:\\n\\n{rule_content}\\n\\nPlease answer this question: {question}\"\n",
    "            }\n",
    "        ]\n",
    "    )[\"message\"][\"content\"]\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Initialize progress bar from current count\n",
    "pbar = tqdm(total=target_count, initial=len(alpaca_dataset), desc=\"Generating dataset\")\n",
    "\n",
    "# Set checkpoint frequency\n",
    "checkpoint_frequency = 20  # Save every 20 entries\n",
    "\n",
    "# Continue processing if we haven't reached our target\n",
    "if len(alpaca_dataset) < target_count:\n",
    "    try:\n",
    "        # First pass: process all rules at least once\n",
    "        for rule in data:\n",
    "            if not rule.get(\"Description\"):\n",
    "                continue  # Skip rules without description\n",
    "                \n",
    "            # Determine how many questions to generate for this rule\n",
    "            question_count = determine_question_count(rule)\n",
    "            \n",
    "            # Generate multiple questions for this rule\n",
    "            questions = generate_contextual_questions(rule, question_count)\n",
    "            \n",
    "            # Process each question\n",
    "            for question in questions:\n",
    "                response = generate_response(rule, question)\n",
    "                \n",
    "                alpaca_dataset.append({\n",
    "                    \"instruction\": question,\n",
    "                    \"input\": \"\",\n",
    "                    \"output\": response\n",
    "                })\n",
    "                \n",
    "                current_count = len(alpaca_dataset)\n",
    "                pbar.update(1)\n",
    "                \n",
    "                # Save checkpoint periodically\n",
    "                if current_count % checkpoint_frequency == 0:\n",
    "                    save_checkpoint(alpaca_dataset, current_count)\n",
    "                \n",
    "                # Check if we've reached our target\n",
    "                if current_count >= target_count:\n",
    "                    break\n",
    "            \n",
    "            # Break the outer loop if we've reached our target\n",
    "            if current_count >= target_count:\n",
    "                break\n",
    "\n",
    "        # If we still don't have enough entries, do a second pass on rules with substantial content\n",
    "        if current_count < target_count:\n",
    "            # Sort rules by content length to prioritize substantial rules\n",
    "            sorted_rules = sorted(data, key=lambda x: len(x.get('Content', '')), reverse=True)\n",
    "            \n",
    "            for rule in sorted_rules:\n",
    "                if not rule.get(\"Description\"):\n",
    "                    continue  # Skip rules without description\n",
    "                    \n",
    "                # Generate additional questions beyond what we did in the first pass\n",
    "                additional_questions = generate_contextual_questions(rule, 2)  # Generate 2 more questions\n",
    "                \n",
    "                for question in additional_questions:\n",
    "                    # Check if this question is too similar to ones we already asked for this rule\n",
    "                    existing_questions = [item[\"instruction\"] for item in alpaca_dataset]\n",
    "                    if any(similar(question, eq) for eq in existing_questions):\n",
    "                        continue  # Skip similar questions\n",
    "                        \n",
    "                    response = generate_response(rule, question)\n",
    "                    \n",
    "                    alpaca_dataset.append({\n",
    "                        \"instruction\": question,\n",
    "                        \"input\": \"\",\n",
    "                        \"output\": response\n",
    "                    })\n",
    "                    \n",
    "                    current_count = len(alpaca_dataset)\n",
    "                    pbar.update(1)\n",
    "                    \n",
    "                    # Save checkpoint periodically\n",
    "                    if current_count % checkpoint_frequency == 0:\n",
    "                        save_checkpoint(alpaca_dataset, current_count)\n",
    "                    \n",
    "                    # Check if we've reached our target\n",
    "                    if current_count >= target_count:\n",
    "                        break\n",
    "                \n",
    "                # Break the outer loop if we've reached our target\n",
    "                if current_count >= target_count:\n",
    "                    break\n",
    "    \n",
    "    except Exception as e:\n",
    "        # If any error occurs, save the checkpoint before exiting\n",
    "        print(f\"Error occurred: {str(e)}\")\n",
    "        save_checkpoint(alpaca_dataset, len(alpaca_dataset))\n",
    "        raise e\n",
    "    \n",
    "    finally:\n",
    "        # Close progress bar\n",
    "        pbar.close()\n",
    "        \n",
    "        # Save final checkpoint\n",
    "        save_checkpoint(alpaca_dataset, len(alpaca_dataset))\n",
    "\n",
    "# Save final dataset to JSON\n",
    "with open(\"kerala_llm_instruction_dataset.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(alpaca_dataset, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Dataset generation complete. Generated {len(alpaca_dataset)} instruction-response pairs. Saved to kerala_llm_instruction_dataset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe86b08e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
