{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e36928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: bert-base-multilingual-cased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name bert-base-multilingual-cased. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KSR Chatbot initialized with multilingual BERT model. Commands:\n",
      "- Type 'quit' to exit\n",
      "- Type 'clear history' to clear conversation history\n",
      "- Type 'show history' to view conversation history\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  paternity leave\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import json\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_community.llms import Ollama\n",
    "import textwrap\n",
    "import torch\n",
    "import os\n",
    "torch.cuda.set_device(6)\n",
    "\n",
    "class RAGChatbot:\n",
    "    def __init__(self, index_path, metadata_path, model_name='bert-base-multilingual-cased'):\n",
    "        # Load FAISS index\n",
    "        self.index = faiss.read_index(index_path)\n",
    "        \n",
    "        # Load metadata\n",
    "        with open(metadata_path, 'r') as f:\n",
    "            self.metadata = json.load(f)\n",
    "            \n",
    "        # Initialize embedding model with mBERT for Malayalam language support\n",
    "        print(f\"Loading embedding model: {model_name}\")\n",
    "        self.embedder = SentenceTransformer(model_name)\n",
    "        \n",
    "        # Initialize Ollama\n",
    "        self.llm = Ollama(model=\"llama3.3:70b-instruct-q8_0\")\n",
    "        \n",
    "        # Initialize conversation history\n",
    "        self.conversation_history = []\n",
    "\n",
    "    def get_relevant_context(self, query, k=6):\n",
    "        # Create query embedding\n",
    "        query_embedding = self.embedder.encode([query])\n",
    "        \n",
    "        # Search in FAISS index\n",
    "        distances, indices = self.index.search(query_embedding.astype('float32'), k)\n",
    "        \n",
    "        # Get relevant texts and their metadata\n",
    "        context = []\n",
    "        for idx in indices[0]:\n",
    "            meta = self.metadata[idx]\n",
    "\n",
    "            # Create a list of field-value pairs, excluding empty values\n",
    "            fields = []\n",
    "            if meta.get('Document'):\n",
    "                fields.append(f\"Document: {meta['Document']}\")\n",
    "            if meta.get('Part'):\n",
    "                fields.append(f\"Part: {meta['Part']}\")\n",
    "            if meta.get('Chapter'):\n",
    "                fields.append(f\"Chapter: {meta['Chapter']}\")\n",
    "            if meta.get('Appendix'):\n",
    "                fields.append(f\"Appendix: {meta['Appendix']}\")\n",
    "            if meta.get('Annexure'):\n",
    "                fields.append(f\"Annexure: {meta['Annexure']}\")\n",
    "            if meta.get('Section'):\n",
    "                fields.append(f\"Section: {meta['Section']}\")\n",
    "            if meta.get('Sub Section'):\n",
    "                fields.append(f\"Sub Section: {meta['Sub Section']}\")\n",
    "            if meta.get('Sub division'):\n",
    "                fields.append(f\"Sub division: {meta['Sub division']}\")\n",
    "            if meta.get('Rule no.'):\n",
    "                fields.append(f\"Rule: {meta['Rule no.']}\")\n",
    "            if meta.get('Amendment order no.'):\n",
    "                fields.append(f\"Amendment Order: {meta['Amendment order no.']}\")\n",
    "            if meta.get('Order date'):\n",
    "                fields.append(f\"Order Date: {meta['Order date']}\")\n",
    "            if meta.get('Effective date'):\n",
    "                fields.append(f\"Effective Date: {meta['Effective date']}\")\n",
    "            if meta.get('Description'):\n",
    "                fields.append(f\"Description: {meta['Description']}\")\n",
    "\n",
    "            # Join all non-empty fields with commas\n",
    "            context_string = ', '.join([f for f in fields if f])\n",
    "            context.append(context_string)\n",
    "        return context\n",
    "    \n",
    "    def generate_response(self, query, context):\n",
    "        # Create prompt with conversation history\n",
    "        conversation_context = \"\\n\".join([\n",
    "            f\"Human: {exchange['query']}\\nAssistant: {exchange['response']}\"\n",
    "            for exchange in self.conversation_history[-3:]  # Include last 3 exchanges\n",
    "        ])\n",
    "        \n",
    "        prompt = f\"\"\"You are an expert assistant in Kerala Government Rules like KSR, KFC, KTC, KSSR etc.  \n",
    "Follow these guidelines for your responses:\n",
    "1. Use simple, everyday language that anyone can understand\n",
    "2. Organize your answer in clear paragraphs with one main idea per paragraph\n",
    "3. Start with the most important information first\n",
    "4. Include proper references (document, part, chapter, section, appendix, annexure, rule number, etc.) when available\n",
    "5. Clearly state if the answer cannot be found in the provided rules\n",
    "6. Avoid technical jargon unless absolutely necessary, and explain any technical terms you must use\n",
    "7. Use short sentences and simple sentence structure\n",
    "8. DO NOT fabricate information. If the answer is not found in the rules, explicitly state so.\n",
    "\n",
    "Previous conversation:\n",
    "{conversation_context}\n",
    "\n",
    "Relevant Rules:\n",
    "{' '.join(context)}\n",
    "\n",
    "Current question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "        # Generate response using Ollama\n",
    "        response = self.llm.invoke(prompt)\n",
    "        \n",
    "        # Update conversation history\n",
    "        self.conversation_history.append({\n",
    "            'query': query,\n",
    "            'response': response,\n",
    "            'context': context\n",
    "        })\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def chat(self, query):\n",
    "        # Handle conversation management commands\n",
    "        if query.lower() == 'clear history':\n",
    "            self.conversation_history = []\n",
    "            return \"Conversation history cleared.\"\n",
    "            \n",
    "        if query.lower() == 'show history':\n",
    "            history = \"\\n\\n\".join([\n",
    "                f\"Human: {exchange['query']}\\nAssistant: {exchange['response']}\"\n",
    "                for exchange in self.conversation_history\n",
    "            ])\n",
    "            return f\"Conversation History:\\n{history}\"\n",
    "        \n",
    "        # Normal query processing\n",
    "        context = self.get_relevant_context(query)\n",
    "        response = self.generate_response(query, context)\n",
    "        return response\n",
    "\n",
    "def main():\n",
    "    # Initialize chatbot with mBERT model\n",
    "    chatbot = RAGChatbot(\n",
    "        '/workspace/Malayalam/vector_db/embeddings.faiss', \n",
    "        '/workspace/Malayalam/vector_db/metadata.json',\n",
    "        model_name='bert-base-multilingual-cased'  # Using mBERT for Malayalam support\n",
    "    )\n",
    "    \n",
    "    print(\"KSR Chatbot initialized with multilingual BERT model. Commands:\")\n",
    "    print(\"- Type 'quit' to exit\")\n",
    "    print(\"- Type 'clear history' to clear conversation history\")\n",
    "    print(\"- Type 'show history' to view conversation history\")\n",
    "    \n",
    "    while True:\n",
    "        query = input(\"\\nYou: \").strip()\n",
    "        \n",
    "        if query.lower() == 'quit':\n",
    "            print(\"\\nGoodbye!\")\n",
    "            break\n",
    "            \n",
    "        try:\n",
    "            response = chatbot.chat(query)\n",
    "            print(\"\\nAssistant:\", textwrap.fill(response, width=80))\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f6cbc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
