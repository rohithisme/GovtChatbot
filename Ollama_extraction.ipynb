{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54c892d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23e4999",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57164c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm_response(prompt):\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    \n",
    "    data = {\n",
    "        \"model\": \"llama3.3:70b-instruct-q8_0\",\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    \n",
    "    response = requests.post(url, json=data)\n",
    "    return response.json()['response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc12596",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests\n",
    "!pip install typing-extensions==4.7.1\n",
    "!pip install pydantic==2.4.2\n",
    "!pip install ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff6135c",
   "metadata": {},
   "source": [
    "KSR Vol 1 - Chunk 2000, Overlap 300"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943af0cb",
   "metadata": {},
   "source": [
    "KSR Vol 1 - Chunk 2000, Overlap 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f8a89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_section_info(text):\n",
    "    patterns = {\n",
    "        'Document': r'Document:\"([^\"]+)\"',\n",
    "        'Part': r'Part:\"([^\"]+)\"',\n",
    "        'Chapter': r'Chapter:\"([^\"]+)\"',\n",
    "        'Appendix': r'Appendix:\"([^\"]+)\"',\n",
    "        'Annexure': r'Annexure:\"([^\"]+)\"',\n",
    "        'Section': r'Section:\"([^\"]+)\"',\n",
    "        'Sub Section': r'Sub Section:\"([^\"]+)\"',\n",
    "        'Sub division': r'Sub division:\"([^\"]+)\"',\n",
    "        'Rule no.': r'Rule no.:\"([^\"]+)\"'\n",
    "    }\n",
    "    \n",
    "    result = {k: None for k in patterns.keys()}\n",
    "    \n",
    "    for key, pattern in patterns.items():\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            result[key] = match.group(1)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def process_text_chunk(text):\n",
    "    # Split into sections based on \"Rule no.:\" pattern\n",
    "    sections = re.split(r'(?=Rule no.:\")', text)\n",
    "    sections = [s for s in sections if s.strip()]\n",
    "\n",
    "    results = []\n",
    "    for section in sections:\n",
    "        # Extract section info and description\n",
    "        section_info = extract_section_info(section)\n",
    "        \n",
    "        # Extract description - everything after the last known field\n",
    "        description_pattern = r'Description:\"([^\"]+)\"'\n",
    "        desc_match = re.search(description_pattern, section)\n",
    "        if desc_match:\n",
    "            section_info['Description'] = desc_match.group(1)\n",
    "        \n",
    "        if any(section_info.values()):\n",
    "            results.append(section_info)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def chunk_text(text, max_length=2000, overlap=300):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    \n",
    "    while start < len(text):\n",
    "        end = start + max_length\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        start = end - overlap\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def process_with_llama(text):\n",
    "    prompt = \"\"\"You are tasked with extracting structured data from a text block that contains information about rules or sections from a document, likely the Kerala Service Rules (KSR). Your goal is to parse this information and format it as a JSON list.\n",
    "\n",
    "The JSON format for each entry should be as follows:\n",
    "{\n",
    "  \"Document\": \"KSR\",\n",
    "  \"Part\": \"\",\n",
    "  \"Chapter\": \"\",\n",
    "  \"Appendix\": \"\",\n",
    "  \"Annexure\": \"\",\n",
    "  \"Section\": \"\",\n",
    "  \"Sub Section\": \"\",\n",
    "  \"Sub division\": \"\",\n",
    "  \"Rule no.\": \"\",\n",
    "  \"Description\": \"\"\n",
    "}\n",
    "\n",
    "Here is the text block you need to parse:\n",
    "\n",
    "<text_block>\n",
    "{?text}\n",
    "</text_block>\n",
    "\n",
    "To extract the required information:\n",
    "\n",
    "1. Identify the document name (usually \"KSR\" unless specified otherwise).\n",
    "2. Look for indicators of Part, Chapter, Appendix, Annexure, Section, Sub Section, Sub division, and Rule number. These may be explicitly stated or implied by the structure of the text.\n",
    "3. The remaining text should be considered as the Description.\n",
    "4. If any field is not present in the text, leave it as an empty string in the JSON.\n",
    "5. If multiple rules or sections are present in the text block, create separate JSON objects for each.\n",
    "6. Ensure that the Rule number is a string, even if it's a number.\n",
    "7. For the Description, include all relevant text, including notes, government decisions, and examples if present.\n",
    "\n",
    "Your output should be a valid JSON list containing one or more objects in the specified format. Do not include any additional text or explanations outside of the JSON structure.\n",
    "\n",
    "<output>\n",
    "</output>\n",
    "\"\"\".replace(\"{?text}\", text)\n",
    "\n",
    "    response = requests.post('http://localhost:11434/api/generate',\n",
    "                           json={\n",
    "                               'model': 'llama3.3:70b-instruct-q8_0',\n",
    "                               'prompt': prompt,\n",
    "                               'stream': False\n",
    "                           })\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        try:\n",
    "            response_json = response.json()['response']\n",
    "            # Extract JSON content from markdown code block if present\n",
    "            if '```json' in response_json:\n",
    "                json_str = response_json.split('```json')[1].split('```')[0].strip()\n",
    "            else:\n",
    "                json_str = response_json.strip()\n",
    "            \n",
    "            # Parse the JSON string\n",
    "            return json.loads(json_str)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing LLM response: {e}\")\n",
    "            print(f\"Response was: {response.json()['response']}\")\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def main():\n",
    "    # Read the input file\n",
    "#     with open('Extracted/KSR_Vol_2_pdfminer_extracted.txt', 'r', encoding='utf-8') as file:\n",
    "#         text = file.read()\n",
    "        \n",
    "    # Open and read both files\n",
    "    with open('Extracted/KSR_Vol_1_pdfminer_extracted.txt', 'r', encoding='utf-8') as file1, \\\n",
    "         open('Extracted/KSR_Vol_2_pdfminer_extracted.txt', 'r', encoding='utf-8') as file2:\n",
    "        text = file1.read() + file2.read()\n",
    "        \n",
    "    \n",
    "    # Process text in chunks\n",
    "    chunks = chunk_text(text)  # Removed the [:3] slice to process all chunks\n",
    "    all_results = []\n",
    "    \n",
    "    for chunk in tqdm(chunks, desc=\"Processing text chunks\"):\n",
    "        # Try direct pattern matching first\n",
    "        results = process_text_chunk(chunk)\n",
    "        \n",
    "        # If pattern matching fails, use LLM\n",
    "        if not results:\n",
    "            llm_result = process_with_llama(chunk)\n",
    "            if llm_result:\n",
    "                all_results.extend(llm_result if isinstance(llm_result, list) else [llm_result])\n",
    "        else:\n",
    "            all_results.extend(results)\n",
    "    \n",
    "    # Add debug output\n",
    "    print(f\"Total results found: {len(all_results)}\")\n",
    "    \n",
    "    # Write results to JSON file\n",
    "    if all_results:\n",
    "        with open('Full_KSR_extracted_rules.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(all_results, f, indent=4, ensure_ascii=False)\n",
    "        print(\"Results successfully written to Full_KSR_extracted_rules.json\")\n",
    "    else:\n",
    "        print(\"No results were found to write to file\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    result = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b61a9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 20 examples for few-shot learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing text chunks: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [01:48<00:00, 21.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total results found: 7\n",
      "Results successfully written to KSR_extracted_rules.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_examples(filename='Data_extraction_format.json'):\n",
    "    \"\"\"Load the example data for few-shot learning.\"\"\"\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def create_few_shot_prompt(examples, text_to_process):\n",
    "    \"\"\"Create a prompt that includes examples and the text to process.\"\"\"\n",
    "    prompt = \"\"\"You are tasked with extracting structured data from text blocks containing information about rules from Kerala Service Rules (KSR). Your goal is to parse this information and format it as a JSON object.\n",
    "\n",
    "Here are 20 examples of correctly formatted extractions:\n",
    "\n",
    "\"\"\"\n",
    "    # Add examples\n",
    "    for idx, example in enumerate(examples, 1):\n",
    "        example_str = json.dumps(example, indent=2)\n",
    "        prompt += f\"Example {idx}:\\n{example_str}\\n\\n\"\n",
    "\n",
    "    prompt += \"\"\"Now, using the same format as the examples above, extract the information from this text:\n",
    "\n",
    "<text_block>\n",
    "{text}\n",
    "</text_block>\n",
    "\n",
    "Important patterns to look for:\n",
    "1. Document is usually \"KSR\"\n",
    "2. Look for Part (I, II, III, etc.)\n",
    "3. Look for Chapter references\n",
    "4. Look for Appendix references\n",
    "5. Look for Annexure references\n",
    "6. Look for Section references\n",
    "7. Look for Sub Section references\n",
    "8. Look for Sub division references\n",
    "9. Look for Rule numbers\n",
    "10. Everything else goes into Description\n",
    "\n",
    "Your output should match the format of the examples exactly. Only output valid JSON, with no additional text or explanations.\n",
    "\"\"\".replace(\"{text}\", text_to_process)\n",
    "\n",
    "    return prompt\n",
    "\n",
    "def process_text_chunk(text, examples):\n",
    "    \"\"\"Process a chunk of text using the LLM with few-shot examples.\"\"\"\n",
    "    prompt = create_few_shot_prompt(examples, text)\n",
    "    \n",
    "    response = requests.post('http://localhost:11434/api/generate',\n",
    "                           json={\n",
    "                               'model': 'llama3.3:70b-instruct-q8_0',\n",
    "                               'prompt': prompt,\n",
    "                               'stream': False\n",
    "                           })\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        try:\n",
    "            response_json = response.json()['response']\n",
    "            # Clean up the response text\n",
    "            response_text = response_json.strip()\n",
    "            \n",
    "            # Handle markdown code blocks\n",
    "            if '```' in response_text:\n",
    "                # Extract content between code blocks, regardless of language specification\n",
    "                response_text = re.search(r'```(?:json)?\\n(.*?)\\n```', response_text, re.DOTALL)\n",
    "                if response_text:\n",
    "                    response_text = response_text.group(1)\n",
    "            \n",
    "            # Remove any leading/trailing whitespace\n",
    "            json_str = response_text.strip()\n",
    "            \n",
    "            try:\n",
    "                # Parse the JSON string\n",
    "                result = json.loads(json_str)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Failed to parse JSON: {e}\")\n",
    "                print(f\"Attempted to parse: {json_str}\")\n",
    "                return None\n",
    "            return result if isinstance(result, list) else [result]\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing LLM response: {e}\")\n",
    "            print(f\"Response was: {response.json()['response']}\")\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def chunk_text(text, max_length=2000, overlap=300):\n",
    "    \"\"\"Split text into chunks with overlap.\"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    \n",
    "    while start < len(text):\n",
    "        # Find the end of the current chunk\n",
    "        end = start + max_length\n",
    "        if end >= len(text):\n",
    "            chunks.append(text[start:])\n",
    "            break\n",
    "            \n",
    "        # Try to find a good breaking point (end of a rule)\n",
    "        next_rule = text[end:].find('Rule no.:')\n",
    "        if next_rule != -1:\n",
    "            end = end + next_rule\n",
    "        \n",
    "        chunks.append(text[start:end])\n",
    "        start = end - overlap\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def main():\n",
    "    # Load the example data\n",
    "    examples = load_examples()\n",
    "    print(f\"Loaded {len(examples)} examples for few-shot learning\")\n",
    "    \n",
    "    # Read the input files\n",
    "    with open('Extracted/KSR_Vol_1_pdfminer_extracted.txt', 'r', encoding='utf-8') as file1, \\\n",
    "         open('Extracted/KSR_Vol_2_pdfminer_extracted.txt', 'r', encoding='utf-8') as file2:\n",
    "        text = file1.read() + file2.read()\n",
    "    \n",
    "    # Process text in chunks\n",
    "    chunks = chunk_text(text)[:5]\n",
    "    all_results = []\n",
    "    \n",
    "    for chunk in tqdm(chunks, desc=\"Processing text chunks\"):\n",
    "        results = process_text_chunk(chunk, examples)\n",
    "        if results:\n",
    "            all_results.extend(results)\n",
    "    \n",
    "    # Add debug output\n",
    "    print(f\"Total results found: {len(all_results)}\")\n",
    "    \n",
    "    # Write results to JSON file\n",
    "    if all_results:\n",
    "        with open('KSR_extracted_rules.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(all_results, f, indent=4, ensure_ascii=False)\n",
    "        print(\"Results successfully written to KSR_extracted_rules.json\")\n",
    "    else:\n",
    "        print(\"No results were found to write to file\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    result = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d214128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 20 examples for few-shot learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing text chunks: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [05:44<00:00, 57.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total results found: 6\n",
      "Results successfully written to KSR_extracted_rules.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_examples(filename='Data_extraction_format.json'):\n",
    "    \"\"\"Load the example data for few-shot learning.\"\"\"\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def normalize_rule_number(rule_text):\n",
    "    \"\"\"Normalize rule number format.\"\"\"\n",
    "    if not rule_text:\n",
    "        return \"\"\n",
    "    # Remove any spaces and convert to string\n",
    "    rule_text = str(rule_text).strip()\n",
    "    return rule_text\n",
    "\n",
    "def create_few_shot_prompt(examples, text_to_process):\n",
    "    \"\"\"Create a prompt that includes examples and the text to process.\"\"\"\n",
    "    prompt = \"\"\"You are tasked with extracting structured information from Kerala Service Rules (KSR) text into a consistent JSON format.\n",
    "\n",
    "Key requirements:\n",
    "1. Always use these exact field names:\n",
    "   - Document\n",
    "   - Part\n",
    "   - Chapter\n",
    "   - Appendix\n",
    "   - Annexure\n",
    "   - Section\n",
    "   - Sub Section\n",
    "   - Sub division\n",
    "   - Rule no.\n",
    "   - Description\n",
    "\n",
    "2. Always include all fields, using empty strings (\"\") for missing values, not null.\n",
    "3. The \"Document\" field should always be \"KSR\" if present in the text.\n",
    "4. Rule numbers should be strings, even if they're numbers.\n",
    "5. Keep the format exactly consistent with these examples.\n",
    "\n",
    "Here are examples of correctly formatted extractions:\n",
    "\n",
    "\"\"\"\n",
    "    # Add examples\n",
    "    for idx, example in enumerate(examples, 1):\n",
    "        example_str = json.dumps(example, indent=2)\n",
    "        prompt += f\"Example {idx}:\\n{example_str}\\n\\n\"\n",
    "\n",
    "    prompt += \"\"\"Now, extract the information from this text using the exact same format as the examples:\n",
    "\n",
    "<text>\n",
    "{text}\n",
    "</text>\n",
    "\n",
    "Remember:\n",
    "- Use empty strings (\"\") for missing fields\n",
    "- Include ALL fields mentioned above\n",
    "- Keep rule numbers as strings\n",
    "- Follow the exact format of the examples\n",
    "- Put all other text in the Description field\n",
    "\n",
    "Output only the JSON object, no other text.\"\"\".replace(\"{text}\", text_to_process)\n",
    "\n",
    "    return prompt\n",
    "\n",
    "def standardize_output(result):\n",
    "    \"\"\"Standardize the JSON output format.\"\"\"\n",
    "    if not isinstance(result, dict):\n",
    "        return None\n",
    "        \n",
    "    # Standard fields that should always be present\n",
    "    standard_fields = {\n",
    "        \"Document\": \"KSR\",\n",
    "        \"Part\": \"\", \n",
    "        \"Chapter\": \"\",\n",
    "        \"Appendix\": \"\",\n",
    "        \"Annexure\": \"\",\n",
    "        \"Section\": \"\",\n",
    "        \"Sub Section\": \"\",\n",
    "        \"Sub division\": \"\",\n",
    "        \"Rule no.\": \"\",\n",
    "        \"Description\": \"\"\n",
    "    }\n",
    "    \n",
    "    # Create a new standardized dictionary\n",
    "    standardized = standard_fields.copy()\n",
    "    \n",
    "    # Map common field variations\n",
    "    field_mappings = {\n",
    "        \"Rule\": \"Rule no.\",\n",
    "        \"rule_no\": \"Rule no.\",\n",
    "        \"rule_number\": \"Rule no.\",\n",
    "    }\n",
    "    \n",
    "    # Collect all description-related fields\n",
    "    description_parts = []\n",
    "    \n",
    "    for key, value in result.items():\n",
    "        # Map the field name if it's a known variation\n",
    "        standard_key = field_mappings.get(key, key)\n",
    "        \n",
    "        # Handle description-related fields\n",
    "        if key.startswith('Description') or key == 'Provided' or key == 'Model_Form':\n",
    "            if value and isinstance(value, str) and value.strip():\n",
    "                if key == 'Provided':\n",
    "                    description_parts.append(f\"Provided that {value}\")\n",
    "                elif key == 'Model_Form':\n",
    "                    description_parts.append(f\"Note: {value}\")\n",
    "                else:\n",
    "                    description_parts.append(value)\n",
    "            continue\n",
    "            \n",
    "        # Skip unknown fields that aren't description-related\n",
    "        if standard_key not in standard_fields:\n",
    "            continue\n",
    "            \n",
    "        # Handle the value\n",
    "        if value is None:\n",
    "            standardized[standard_key] = \"\"\n",
    "        elif isinstance(value, (list, dict)):\n",
    "            # Convert complex rule structures to string\n",
    "            if standard_key == \"Rule no.\":\n",
    "                if isinstance(value, list):\n",
    "                    standardized[standard_key] = \", \".join(str(v) for v in value)\n",
    "                elif isinstance(value, dict):\n",
    "                    standardized[standard_key] = str(value.get(\"Number\", \"\"))\n",
    "        else:\n",
    "            standardized[standard_key] = str(value).strip()\n",
    "    \n",
    "    # Combine all description parts into main description\n",
    "    if description_parts:\n",
    "        if standardized[\"Description\"]:\n",
    "            description_parts.insert(0, standardized[\"Description\"])\n",
    "        standardized[\"Description\"] = \"\\n\\n\".join(filter(None, description_parts))\n",
    "            \n",
    "    return standardized\n",
    "\n",
    "def process_text_chunk(text, examples):\n",
    "    \"\"\"Process a chunk of text using the LLM with few-shot examples.\"\"\"\n",
    "    prompt = create_few_shot_prompt(examples, text)\n",
    "    \n",
    "    response = requests.post('http://localhost:11434/api/generate',\n",
    "                           json={\n",
    "                               'model': 'qwen2.5:72b-instruct-q8_0',\n",
    "                               'prompt': prompt,\n",
    "                               'stream': False\n",
    "                           })\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        try:\n",
    "            response_json = response.json()['response']\n",
    "            # Clean up the response text\n",
    "            response_text = response_json.strip()\n",
    "            \n",
    "            # Handle markdown code blocks\n",
    "            if '```' in response_text:\n",
    "                # Extract content between code blocks, regardless of language specification\n",
    "                response_text = re.search(r'```(?:json)?\\n(.*?)\\n```', response_text, re.DOTALL)\n",
    "                if response_text:\n",
    "                    response_text = response_text.group(1)\n",
    "            \n",
    "            # Remove any leading/trailing whitespace\n",
    "            response_text = response_text.strip()\n",
    "            \n",
    "            # Handle multiple JSON objects in response\n",
    "            # Split on closing brace followed by opening brace\n",
    "            json_objects = re.split(r'}\\s*{', response_text)\n",
    "            \n",
    "            result = []\n",
    "            for idx, json_obj in enumerate(json_objects):\n",
    "                # Add back the braces except for first and last object\n",
    "                if idx > 0:\n",
    "                    json_obj = '{' + json_obj\n",
    "                if idx < len(json_objects) - 1:\n",
    "                    json_obj = json_obj + '}'\n",
    "                    \n",
    "                try:\n",
    "                    # Parse individual JSON object\n",
    "                    parsed_obj = json.loads(json_obj)\n",
    "                    result.append(parsed_obj)\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Failed to parse JSON object {idx + 1}: {e}\")\n",
    "                    print(f\"Object text: {json_obj}\")\n",
    "                    continue\n",
    "            \n",
    "            # Handle both single objects and lists\n",
    "            if isinstance(result, dict):\n",
    "                result = [result]\n",
    "            \n",
    "            # Standardize each result\n",
    "            standardized_results = []\n",
    "            for item in result:\n",
    "                standardized = standardize_output(item)\n",
    "                if standardized:\n",
    "                    standardized_results.append(standardized)\n",
    "            \n",
    "            return standardized_results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing LLM response: {e}\")\n",
    "            print(f\"Response was: {response_json}\")\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def chunk_text(text, max_length=2000, overlap=300):\n",
    "    \"\"\"Split text into chunks with overlap.\"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    \n",
    "    while start < len(text):\n",
    "        # Find the end of the current chunk\n",
    "        end = start + max_length\n",
    "        if end >= len(text):\n",
    "            chunks.append(text[start:])\n",
    "            break\n",
    "            \n",
    "        # Try to find a good breaking point (end of a rule)\n",
    "        next_rule = text[end:].find('Rule no.:')\n",
    "        if next_rule != -1:\n",
    "            end = end + next_rule\n",
    "        \n",
    "        chunks.append(text[start:end])\n",
    "        start = end - overlap\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def main():\n",
    "    # Load the example data\n",
    "    examples = load_examples()\n",
    "    print(f\"Loaded {len(examples)} examples for few-shot learning\")\n",
    "    \n",
    "    # Read the input files\n",
    "    with open('Extracted/KSR_Vol_1_pdfminer_extracted.txt', 'r', encoding='utf-8') as file1, \\\n",
    "         open('Extracted/KSR_Vol_2_pdfminer_extracted.txt', 'r', encoding='utf-8') as file2:\n",
    "        text = file1.read() + file2.read()\n",
    "    \n",
    "    # Process text in chunks\n",
    "    chunks = chunk_text(text)[55:61]\n",
    "    all_results = []\n",
    "    \n",
    "    for chunk in tqdm(chunks, desc=\"Processing text chunks\"):\n",
    "        results = process_text_chunk(chunk, examples)\n",
    "        if results:\n",
    "            all_results.extend(results)\n",
    "    \n",
    "    # Add debug output\n",
    "    print(f\"Total results found: {len(all_results)}\")\n",
    "    \n",
    "    # Write results to JSON file\n",
    "    if all_results:\n",
    "        with open('KSR_extracted_rules.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(all_results, f, indent=4, ensure_ascii=False)\n",
    "        print(\"Results successfully written to KSR_extracted_rules.json\")\n",
    "    else:\n",
    "        print(\"No results were found to write to file\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    result = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
