{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "378f9674",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: langchain-community in /usr/local/lib/python3.8/dist-packages (0.2.19)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.8/dist-packages (from langchain-community) (6.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.8/dist-packages (from langchain-community) (2.0.37)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.8/dist-packages (from langchain-community) (3.10.11)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.8/dist-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: langchain<0.3.0,>=0.2.17 in /usr/local/lib/python3.8/dist-packages (from langchain-community) (0.2.17)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.43 in /usr/local/lib/python3.8/dist-packages (from langchain-community) (0.2.43)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.112 in /usr/local/lib/python3.8/dist-packages (from langchain-community) (0.1.147)\n",
      "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.8/dist-packages (from langchain-community) (1.22.2)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.8/dist-packages (from langchain-community) (2.28.1)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from langchain-community) (8.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (22.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.15.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.8/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.22.0)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from langchain<0.3.0,>=0.2.17->langchain-community) (0.2.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.8/dist-packages (from langchain<0.3.0,>=0.2.17->langchain-community) (2.10.6)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.8/dist-packages (from langchain-core<0.3.0,>=0.2.43->langchain-community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.8/dist-packages (from langchain-core<0.3.0,>=0.2.43->langchain-community) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.8/dist-packages (from langchain-core<0.3.0,>=0.2.43->langchain-community) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from langsmith<0.2.0,>=0.1.112->langchain-community) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.8/dist-packages (from langsmith<0.2.0,>=0.1.112->langchain-community) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from langsmith<0.2.0,>=0.1.112->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2->langchain-community) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2->langchain-community) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2->langchain-community) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2->langchain-community) (2022.9.24)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.8/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.8/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-community) (4.5.2)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.8/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-community) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.8/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-community) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.8/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.43->langchain-community) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.17->langchain-community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.8/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.17->langchain-community) (2.27.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain-community) (0.2.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.8/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-community) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.8/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-community) (1.0.4)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6b9ed21",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.8/dist-packages (1.8.0.post1)\n",
      "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.8/dist-packages (3.2.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (1.22.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from faiss-cpu) (21.3)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (4.46.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (4.64.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.13.0a0+936e930)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (0.24.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.6.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (0.28.0)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (9.0.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.12.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.28.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->faiss-cpu) (3.0.9)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2022.10.31)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.8/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2022.9.24)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install faiss-cpu sentence-transformers numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b63bcc06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e987fb20b3174134a866dc2a3d1f5dba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/69 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created FAISS index with 2191 documents\n",
      "Index saved to Vector_DB/document_index.faiss\n",
      "Metadata saved to Vector_DB/metadata.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "\n",
    "def load_documents(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        documents = json.load(f)\n",
    "    return documents\n",
    "\n",
    "def prepare_texts(documents):\n",
    "    # Combine relevant fields for each document\n",
    "    texts = []\n",
    "    metadata = []\n",
    "    \n",
    "    for doc in documents:\n",
    "        # Combine fields into a single text\n",
    "        text_parts = [\n",
    "            doc.get('Document', ''),\n",
    "            doc.get('Description', ''),\n",
    "            f\"Rule {doc.get('Rule no.', '')}\" if doc.get('Rule no.') else ''\n",
    "        ]\n",
    "        text = ' '.join(filter(None, text_parts))\n",
    "        \n",
    "        if text.strip():\n",
    "            texts.append(text)\n",
    "            metadata.append({\n",
    "                'Document': doc.get('Document', ''),\n",
    "                'Rule_no': doc.get('Rule no.', ''),\n",
    "                'Part': doc.get('Part', ''),\n",
    "                'Chapter': doc.get('Chapter', '')\n",
    "            })\n",
    "    \n",
    "    return texts, metadata\n",
    "\n",
    "def create_embeddings(texts, model_name='all-MiniLM-L6-v2', batch_size=32):\n",
    "    # Initialize the transformer model with device='cpu'\n",
    "    model = SentenceTransformer(model_name, device='cpu')\n",
    "    \n",
    "    # Generate embeddings in batches\n",
    "    embeddings = model.encode(texts, show_progress_bar=True, batch_size=batch_size)\n",
    "    return embeddings\n",
    "\n",
    "def create_faiss_index(embeddings, save_dir):\n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Initialize FAISS index\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    \n",
    "    # Add vectors to the index\n",
    "    index.add(np.array(embeddings).astype('float32'))\n",
    "    \n",
    "    # Save the index\n",
    "    faiss.write_index(index, os.path.join(save_dir, 'document_index.faiss'))\n",
    "    return index\n",
    "\n",
    "def save_metadata(metadata, save_dir):\n",
    "    with open(os.path.join(save_dir, 'metadata.json'), 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "\n",
    "def main():\n",
    "    # Paths\n",
    "    input_file = '/workspace/rohith_llm/Full_KSR_extracted_rules.json'\n",
    "    save_dir = 'Vector_DB'\n",
    "    \n",
    "    # Load and prepare documents\n",
    "    documents = load_documents(input_file)\n",
    "    texts, metadata = prepare_texts(documents)\n",
    "    \n",
    "    # Create embeddings\n",
    "    embeddings = create_embeddings(texts)\n",
    "    \n",
    "    # Create and save FAISS index\n",
    "    index = create_faiss_index(embeddings, save_dir)\n",
    "    \n",
    "    # Save metadata\n",
    "    save_metadata(metadata, save_dir)\n",
    "    \n",
    "    print(f\"Created FAISS index with {len(texts)} documents\")\n",
    "    print(f\"Index saved to {save_dir}/document_index.faiss\")\n",
    "    print(f\"Metadata saved to {save_dir}/metadata.json\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100888ef",
   "metadata": {},
   "source": [
    "KSR Amendments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "020762f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for 12 new documents...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39debad49781451e9f46c35f38d828f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding new embeddings to FAISS index...\n",
      "Verifying index consistency...\n",
      "Saving updated index and metadata...\n",
      "Successfully added 12 new documents to the database\n",
      "Total documents in database: 2215\n",
      "Performing index integrity check...\n",
      "Number of vectors in FAISS index: 2215\n",
      "Number of entries in metadata: 2215\n",
      "Database update completed successfully with index integrity maintained\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import json\n",
    "import os\n",
    "\n",
    "def load_existing_index(index_path):\n",
    "    \"\"\"Load existing FAISS index\"\"\"\n",
    "    return faiss.read_index(index_path)\n",
    "\n",
    "def load_existing_metadata(metadata_path):\n",
    "    \"\"\"Load existing metadata\"\"\"\n",
    "    with open(metadata_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def process_go_document(content, index):\n",
    "    \"\"\"Process a government order document and extract relevant information\"\"\"\n",
    "    # Extract basic information\n",
    "    lines = content.split('\\n')\n",
    "    go_number = \"\"\n",
    "    date = \"\"\n",
    "    department = \"\"\n",
    "    \n",
    "    for line in lines:\n",
    "        if \"Government Order (P) No.\" in line:\n",
    "            go_number = line.strip()\n",
    "        elif \"Date:\" in line:\n",
    "            date = line.split(\"Date:\")[1].strip()\n",
    "        elif \"Department:\" in line:\n",
    "            department = line.split(\"Department:\")[1].strip()\n",
    "            break\n",
    "    \n",
    "    return {\n",
    "        'Document': go_number,\n",
    "        'Date': date,\n",
    "        'Department': department,\n",
    "        'Content': content,\n",
    "        'Index': index  # Add index number\n",
    "    }\n",
    "\n",
    "def update_faiss_database(existing_index_path, existing_metadata_path, new_documents_folder, save_dir):\n",
    "    \"\"\"Update existing FAISS database with new documents\"\"\"\n",
    "    \n",
    "    # Load existing index and metadata\n",
    "    index = load_existing_index(existing_index_path)\n",
    "    metadata = load_existing_metadata(existing_metadata_path)\n",
    "    \n",
    "    # Get the current number of vectors in the index\n",
    "    current_index_size = index.ntotal\n",
    "    \n",
    "    # Initialize the transformer model\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2', device='cpu')\n",
    "    \n",
    "    # Process new documents\n",
    "    new_texts = []\n",
    "    new_metadata = []\n",
    "    \n",
    "    # Read and process each file in the new documents folder\n",
    "    for i, filename in enumerate(os.listdir(new_documents_folder)):\n",
    "        if filename.endswith('.txt'):\n",
    "            file_path = os.path.join(new_documents_folder, filename)\n",
    "            with open(file_path, 'r') as f:\n",
    "                content = f.read()\n",
    "                \n",
    "                # Process the document with new index number\n",
    "                new_index = current_index_size + i\n",
    "                doc_info = process_go_document(content, new_index)\n",
    "                \n",
    "                # Add to new texts and metadata\n",
    "                new_texts.append(content)\n",
    "                new_metadata.append({\n",
    "                    'Document': doc_info['Document'],\n",
    "                    'Date': doc_info['Date'],\n",
    "                    'Department': doc_info['Department'],\n",
    "                    'Type': 'Government Order',\n",
    "                    'Filename': filename,\n",
    "                    'Index': new_index  # Store the index in metadata\n",
    "                })\n",
    "    \n",
    "    # Generate embeddings for new documents\n",
    "    print(f\"Generating embeddings for {len(new_texts)} new documents...\")\n",
    "    new_embeddings = model.encode(new_texts, show_progress_bar=True, batch_size=32)\n",
    "    \n",
    "    # Add new embeddings to the index\n",
    "    print(\"Adding new embeddings to FAISS index...\")\n",
    "    index.add(np.array(new_embeddings).astype('float32'))\n",
    "    \n",
    "    # Update metadata\n",
    "    # First, ensure existing metadata has index numbers if they don't exist\n",
    "    for i, item in enumerate(metadata):\n",
    "        if 'Index' not in item:\n",
    "            item['Index'] = i\n",
    "    \n",
    "    metadata.extend(new_metadata)\n",
    "    \n",
    "    # Sort metadata by index number\n",
    "    metadata = sorted(metadata, key=lambda x: x['Index'])\n",
    "    \n",
    "    # Verify index consistency\n",
    "    print(\"Verifying index consistency...\")\n",
    "    if index.ntotal != len(metadata):\n",
    "        raise ValueError(f\"Mismatch between index size ({index.ntotal}) and metadata length ({len(metadata)})\")\n",
    "    \n",
    "    # Save updated index and metadata\n",
    "    print(\"Saving updated index and metadata...\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    faiss.write_index(index, os.path.join(save_dir, 'document_index.faiss'))\n",
    "    with open(os.path.join(save_dir, 'metadata.json'), 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"Successfully added {len(new_texts)} new documents to the database\")\n",
    "    print(f\"Total documents in database: {index.ntotal}\")\n",
    "    return index, metadata\n",
    "\n",
    "def verify_index_integrity(index_path, metadata_path):\n",
    "    \"\"\"Verify the integrity of the index and metadata\"\"\"\n",
    "    index = load_existing_index(index_path)\n",
    "    metadata = load_existing_metadata(metadata_path)\n",
    "    \n",
    "    print(\"Performing index integrity check...\")\n",
    "    print(f\"Number of vectors in FAISS index: {index.ntotal}\")\n",
    "    print(f\"Number of entries in metadata: {len(metadata)}\")\n",
    "    \n",
    "    # Check if all metadata entries have unique indices\n",
    "    indices = [item['Index'] for item in metadata]\n",
    "    unique_indices = set(indices)\n",
    "    if len(indices) != len(unique_indices):\n",
    "        print(\"WARNING: Duplicate indices found in metadata!\")\n",
    "    \n",
    "    # Check if indices are continuous\n",
    "    expected_indices = set(range(len(metadata)))\n",
    "    if unique_indices != expected_indices:\n",
    "        print(\"WARNING: Non-continuous indices found!\")\n",
    "        print(\"Missing indices:\", expected_indices - unique_indices)\n",
    "        print(\"Extra indices:\", unique_indices - expected_indices)\n",
    "    \n",
    "    return index.ntotal == len(metadata)\n",
    "\n",
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Paths\n",
    "    existing_index_path = 'Vector_DB/document_index.faiss'\n",
    "    existing_metadata_path = 'Vector_DB/metadata.json'\n",
    "    new_documents_folder = '/workspace/rohith_llm/Documents/Structured amendments/'\n",
    "    save_dir = 'Vector_DB'\n",
    "    \n",
    "    # Update database\n",
    "    index, metadata = update_faiss_database(\n",
    "        existing_index_path,\n",
    "        existing_metadata_path,\n",
    "        new_documents_folder,\n",
    "        save_dir\n",
    "    )\n",
    "    \n",
    "    # Verify the updated database\n",
    "    is_valid = verify_index_integrity(\n",
    "        os.path.join(save_dir, 'document_index.faiss'),\n",
    "        os.path.join(save_dir, 'metadata.json')\n",
    "    )\n",
    "    \n",
    "    if is_valid:\n",
    "        print(\"Database update completed successfully with index integrity maintained\")\n",
    "    else:\n",
    "        print(\"WARNING: Database update completed but index integrity check failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f550881c",
   "metadata": {},
   "source": [
    "COMPLETE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ff5eae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded: /workspace/rohith_llm/Extracted/Structured/KSR.json\n",
      "Successfully loaded: /workspace/rohith_llm/Extracted/Structured/KSR_Amendments.json\n",
      "Successfully loaded: /workspace/rohith_llm/Extracted/Structured/KTC.json\n",
      "Successfully loaded: /workspace/rohith_llm/Extracted/Structured/KFC.json\n",
      "Creating embeddings for 5858 documents...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "186bbb1dae7940359e5706777a821293",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/184 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created FAISS index with 5858 documents\n",
      "Index saved to Vector_DB/document_index.faiss\n",
      "Metadata saved to Vector_DB/metadata.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "def load_documents_from_directory(directory_path):\n",
    "    \"\"\"Load documents from all JSON files in the specified directory\"\"\"\n",
    "    all_documents = []\n",
    "    \n",
    "    # Get all JSON files in the directory\n",
    "    json_files = glob(os.path.join(directory_path, '*.json'))\n",
    "    \n",
    "    for file_path in json_files:\n",
    "        try:\n",
    "            with open(file_path, 'r') as f:\n",
    "                documents = json.load(f)\n",
    "                if isinstance(documents, list):\n",
    "                    all_documents.extend(documents)\n",
    "                print(f\"Successfully loaded: {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_path}: {str(e)}\")\n",
    "    \n",
    "    return all_documents\n",
    "\n",
    "def prepare_texts(documents):\n",
    "    \"\"\"Combine relevant fields for each document\"\"\"\n",
    "    texts = []\n",
    "    metadata = []\n",
    "    \n",
    "    for doc in documents:\n",
    "        # Combine fields into a single text\n",
    "        text_parts = [\n",
    "            doc.get('Document', ''),\n",
    "            doc.get('Description', ''),\n",
    "            f\"Rule {doc.get('Rule no.', '')}\" if doc.get('Rule no.') else ''\n",
    "            \n",
    "        ]\n",
    "        text = ' '.join(filter(None, text_parts))\n",
    "        \n",
    "        if text.strip():\n",
    "            texts.append(text)\n",
    "            metadata.append({\n",
    "                'Document': doc.get('Document', ''),\n",
    "                'Rule_no': doc.get('Rule no.', ''),\n",
    "                'Part': doc.get('Part', ''),\n",
    "                'Chapter': doc.get('Chapter', ''),\n",
    "                'Amendment_order': doc.get('Amendment order no.', ''),\n",
    "                'Order_date': doc.get('Order date', ''),\n",
    "                'Effective_date': doc.get('Effective date', '')\n",
    "            })\n",
    "    \n",
    "    return texts, metadata\n",
    "\n",
    "def create_embeddings(texts, model_name='all-MiniLM-L6-v2', batch_size=32):\n",
    "    \"\"\"Generate embeddings using the specified model\"\"\"\n",
    "    model = SentenceTransformer(model_name, device='cpu')\n",
    "    embeddings = model.encode(texts, show_progress_bar=True, batch_size=batch_size)\n",
    "    return embeddings\n",
    "\n",
    "def create_faiss_index(embeddings, save_dir):\n",
    "    \"\"\"Create and save FAISS index\"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(np.array(embeddings).astype('float32'))\n",
    "    \n",
    "    faiss.write_index(index, os.path.join(save_dir, 'document_index.faiss'))\n",
    "    return index\n",
    "\n",
    "def save_metadata(metadata, save_dir):\n",
    "    \"\"\"Save metadata to JSON file\"\"\"\n",
    "    with open(os.path.join(save_dir, 'metadata.json'), 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "\n",
    "def main():\n",
    "    # Paths\n",
    "    input_directory = '/workspace/rohith_llm/Extracted/Structured'\n",
    "    save_dir = 'Vector_DB'\n",
    "    \n",
    "    # Load and prepare documents from all JSON files\n",
    "    documents = load_documents_from_directory(input_directory)\n",
    "    texts, metadata = prepare_texts(documents)\n",
    "    \n",
    "    if not texts:\n",
    "        print(\"No documents found to process!\")\n",
    "        return\n",
    "    \n",
    "    # Create embeddings\n",
    "    print(f\"Creating embeddings for {len(texts)} documents...\")\n",
    "    embeddings = create_embeddings(texts)\n",
    "    \n",
    "    # Create and save FAISS index\n",
    "    index = create_faiss_index(embeddings, save_dir)\n",
    "    \n",
    "    # Save metadata\n",
    "    save_metadata(metadata, save_dir)\n",
    "    \n",
    "    print(f\"Created FAISS index with {len(texts)} documents\")\n",
    "    print(f\"Index saved to {save_dir}/document_index.faiss\")\n",
    "    print(f\"Metadata saved to {save_dir}/metadata.json\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
